{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install azure-storage-blob\n",
    "\n",
    "%pip install pdfplumber\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "%pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE HAS THE AZURE BLOB STUP WHICH I CAN'T SHARE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63310f68-2491-4499-b74a-1e67f85acfa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_text(binary_data):\n",
    "  pdf_file = BytesIO(binary_data)\n",
    "\n",
    "  with pdfplumber.open(pdf_file) as pdf:\n",
    "    plain_text = \"\"\n",
    "\n",
    "    for page in pdf.pages:\n",
    "      plain_text += page.extract_text()\n",
    "\n",
    "    return plain_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d455e56-8bff-4284-9da6-325e158834a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"] \n",
    "\n",
    "extracted_text = extract_text(binary_pdf) \n",
    "#print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28997c35-c36a-4678-8ad5-954d2f239c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "#https://www.youtube.com/watch?v=dXxQ0LR-3Hg&list=LL&index=1&t=1820s to explain the function\n",
    "\n",
    "\n",
    "def token_chunks(text: str, nlp, tokens_per_chunk: int = 200, overlap: int = 50) -> List[str]:\n",
    "    # Process with spaCy to get tokens\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text_with_ws for token in doc]\n",
    "    \n",
    "    # Create chunks with overlap\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), tokens_per_chunk - overlap):\n",
    "        # Skip tiny final chunks\n",
    "        if i + tokens_per_chunk >= len(tokens) and i > 0:\n",
    "            # Add remaining tokens to last chunk\n",
    "            chunks[-1] += ''.join(tokens[i:])\n",
    "            break\n",
    "            \n",
    "        # Create a new chunk\n",
    "        chunk = ''.join(tokens[i:i + tokens_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "  text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0, \n",
    "    separator=\"\\n\", \n",
    "    length_function=len\n",
    "    )\n",
    "  chunks = text_splitter.split_text(text)\n",
    "  return chunks\n",
    "\n",
    "\n",
    "def get_text_chunks2(text):\n",
    "    chunks = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    return chunks\n",
    "\n",
    "def get_text_chunks3(text):\n",
    "  text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    separator=\"\\n\", \n",
    "    length_function=len\n",
    "    )\n",
    "  chunks = text_splitter.split_text(text)\n",
    "  return chunks\n",
    "\n",
    "def clean_text(text):\n",
    "  if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "  cleaned_text = \" \".join(text.split())\n",
    "  cleaned_text = cleaned_text.lower()\n",
    "  return cleaned_text\n",
    "\n",
    "def lower_text(text):\n",
    "  lowertext = text.lower()\n",
    "  return lowertext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "178c124e-638d-4cdc-8e5e-017b070277dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_pdf_text(extracted_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process extracted PDF text by chunking and cleaning.\n",
    "    \n",
    "    Args:\n",
    "        extracted_text: Raw text extracted from PDF\n",
    "        \n",
    "    Returns:\n",
    "        List of cleaned text chunks\n",
    "    \"\"\"\n",
    "    chunks = get_text_chunks(extracted_text)\n",
    "    \n",
    "    # Clean each chunk\n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "        \n",
    "    return \"\\n\".join(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db399d6-3f52-428e-95f2-e63250f84a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_text = process_pdf_text(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3986cdb-00d4-4b0e-9ceb-4fa448c6eb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of chunks:\", len(main_text))\n",
    "for i, chunk in enumerate(main_text, 1):\n",
    "    print(f\"\\nCHUNK {i}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a2a27c-3bcb-487e-b0db-ca76fbe69a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test the cleaning function\n",
    "test_text = \"\"\"\n",
    "The TR 2144       M:3952 document specifies requirements.\n",
    "Also see MCE 1234:123 and TR 2144 M for details!.\n",
    "Some MCE1234B document and TR 2144M reference.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(clean_text(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4d8a89-ba7a-4248-bde8-1cde44ca3542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''USEFUL LINKS\n",
    "https://www.babelstreet.com/blog/what-is-entity-extraction#:~:text=Entity%20extraction%20(aka%2C%20named%20entity,%2C%20webpages%2C%20text%20fields).\n",
    "\n",
    "https://medium.com/@sanskrutikhedkar09/mastering-information-extraction-from-unstructured-text-a-deep-dive-into-named-entity-recognition-4aa2f664a453\n",
    "\n",
    "https://www.microfocus.com/documentation/relativity/relativity1217/reldbdsn/GUID-7C2DF185-41A1-4448-81E7-3252AA8DEBB3.html \n",
    "\n",
    "'''\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def create_nlp_pipeline():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    \n",
    "    patterns = [\n",
    "        {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\"]}},\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}$\"}},\n",
    "        {\"LOWER\": {\"IN\": [\":\"]}},  # Must have colon immediately after\n",
    "    ]},\n",
    "    # Pattern 1: Connected with optional letter (mce0107b or mce0107)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(mce|mch|mcx|mcg|tr|mcs|oa|og|os|pa|pt|pl|re|rg|se|trg|trh)\\d{4}[a-z]?$\"}}\n",
    "    ]},\n",
    "\n",
    "    # Pattern 2: Space after prefix (MCE 0107B or MCE 0107)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        # Match the prefix more flexibly\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\", \"mcs\", \"oa\", \"og\", \"os\", \"pa\", \"pt\", \"pl\", \"re\", \"rg\", \"se\", \"trg\", \"trh\"]}},\n",
    "        # Match any numbers with optional suffix, removing strict boundaries\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}[A-Za-z]?$\"}}\n",
    "    ]},\n",
    "\n",
    "    # Pattern 3: Prefix, number, and separate letter (mce 0107 b)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\", \"mcs\", \"oa\", \"og\", \"os\", \"pa\", \"pt\", \"pl\", \"re\", \"rg\", \"se\", \"trg\", \"trh\"]}},\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}$\"}},\n",
    "        {\"LOWER\": {\"REGEX\": r\"^[a-z]$\"}},\n",
    "        \n",
    "    ]},\n",
    "\n",
    "            {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\", \"mcs\", \"oa\", \"og\", \"os\", \"pa\", \"pt\", \"pl\", \"re\", \"rg\", \"se\", \"trg\", \"trh\"]}},\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}(?=\\\\)\"}}  # Matches only the 4 digits when followed by backslash\n",
    "    ]},\n",
    "\n",
    "    \n",
    "    # SYSTEM_COMPONENT Patterns\n",
    "    {\"label\": \"SYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(midas|nmcs2?|hadecs|hatms)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"midas\"}, \n",
    "        {\"LOWER\": \"gold\"}\n",
    "    ]},\n",
    "    \n",
    "    # HARDWARE_COMPONENT Patterns\n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(cabinet|plinth|lantern|post|frame|skirt)$\"}}, \n",
    "        {\"LOWER\": \"type\"}, \n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d+[a-z]?$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(ms[1-4]r?|ami|ert)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"indicator\", \"signal\", \"sensor\", \"detector\", \"camera\", \"telephone\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # COMMUNICATION_COMPONENT Patterns\n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(rs485|rs422|tcp\\/ip|lan|wan)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"rs\"}, \n",
    "        {\"TEXT\": {\"REGEX\": r\"^(485|422)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"ethernet\"}, \n",
    "        {\"LOWER\": {\"IN\": [\"lan\", \"connection\", \"interface\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # SUBSYSTEM_COMPONENT Patterns\n",
    "    {\"label\": \"SUBSYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"signal\", \"message\", \"meteorological\", \"tidal\", \"tunnel\"]}}, \n",
    "        {\"LOWER\": \"subsystem\"}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SUBSYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(lcc|pdu|cobs|ceclb|ceceb|cecr)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    # CONTROL_COMPONENT Patterns\n",
    "    {\"label\": \"CONTROL_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"control\", \"monitoring\", \"outstation\", \"instation\"]}}, \n",
    "        {\"LOWER\": {\"IN\": [\"system\", \"unit\", \"equipment\", \"interface\"]}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"CONTROL_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"cctv\"}, \n",
    "        {\"LOWER\": {\"IN\": [\"system\", \"camera\", \"equipment\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # SPECIFICATION_TYPE Patterns\n",
    "    {\"label\": \"SPECIFICATION_TYPE\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"requirements\", \"specification\", \"instructions\", \"overview\", \"process\"]}}, \n",
    "        {\"LOWER\": \"document\"}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SPECIFICATION_TYPE\", \"pattern\": [\n",
    "        {\"LOWER\": \"technical\"}, \n",
    "        {\"LOWER\": \"requirements\"}\n",
    "    ]}\n",
    "]\n",
    "    \n",
    "    ruler.add_patterns(patterns)\n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a653db-fdd4-4727-b9d8-5064e4c684eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def debug_document_code(nlp, text):\n",
    "    \"\"\"\n",
    "    Provides detailed analysis of how document codes are being processed.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing document code: '{text}'\")\n",
    "    \n",
    "    # shows raw tokenization\n",
    "    doc = nlp(text)\n",
    "    print(\"\\nTokenization details:\")\n",
    "    for token in doc:\n",
    "        print(f\"Token: '{token.text}'\")\n",
    "        print(f\"  Position: {token.idx} to {token.idx + len(token.text)}\")\n",
    "        print(f\"  Is part of entity: {token.ent_type_ != ''}\")\n",
    "        print(f\"  Entity type: {token.ent_type_ if token.ent_type_ else 'None'}\")\n",
    "        print()\n",
    "    \n",
    "    # complete entities found\n",
    "    print(\"\\nComplete entities found:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Entity: '{ent.text}'\")\n",
    "        print(f\"  Label: {ent.label_}\")\n",
    "        print(f\"  Includes all tokens: {all(t.ent_type_ == ent.label_ for t in ent)}\")\n",
    "        print()\n",
    "    \n",
    "    # what didn't match\n",
    "    unmatched = [t.text for t in doc if not t.ent_type_]\n",
    "    if unmatched:\n",
    "        print(\"\\nUnmatched tokens:\")\n",
    "        print(\", \".join(unmatched))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee256f9-7b31-482e-a82f-517531652358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "small_chunk = \"\"\"\n",
    "re 1110 The MCE0107B document connects to RS485 while MCH 1070B uses RS 422.\n",
    "MIDAS Gold system interfaces with the Ethernet LAN through Cabinet Type 600.\n",
    "The Signal Subsystem monitors the CCTV System and AMI-EE devices. (AMI bobo)\n",
    "\"\"\"\n",
    "\n",
    "medium_chunk = \"\"\"\n",
    "The assembly manual for MCH0107B specifies that RS485 components must be configured \n",
    "alongside RS422 adapters, with additional references to OA 0150C outlined in \n",
    "Section 4.3 of the document. TR 2043 further details the integration with \n",
    "Ethernet LAN systems, enabling high-speed communication protocols compliant \n",
    "with IEEE 802.3 standards. The MIDAS Gold system connects through Cabinet Type 600A \n",
    "to the Control System, while monitoring occurs via the CCTV System and Signal Subsystem.\n",
    "\n",
    "The MCE 1070B requirements document describes interfacing with NMCS2 through standard \n",
    "protocols. Local connections use RS 485 for primary communication, supported by \n",
    "AMI-EE devices and monitored by the Outstation Equipment.\n",
    "\"\"\"\n",
    "\n",
    "large_chunk = \"\"\"\n",
    "Technical Requirements Document: System Integration Specification\n",
    "\n",
    "1. Overview\n",
    "The MCE0107B specification, in conjunction with MCH 1070B and TR 2043, defines the \n",
    "integration requirements for the MIDAS Gold system. Primary communication occurs through \n",
    "RS485 interfaces, while secondary protocols utilize RS 422 and Ethernet LAN connections.\n",
    "\n",
    "2. Hardware Components\n",
    "Cabinet Type 600 houses the main control units, with additional Cabinet Type 450A units \n",
    "for auxiliary systems. The AMI-EE devices interface with MS3R indicators and standard \n",
    "AMI units. Signal sensors and detector units provide environmental monitoring capabilities.\n",
    "\n",
    "3. System Architecture\n",
    "The NMCS2 framework integrates with HADECS and HATMS subsystems through standardized \n",
    "interfaces. The Signal Subsystem and Message Subsystem handle primary control operations, \n",
    "while the Meteorological Subsystem provides environmental data. CECLB and CECEB units \n",
    "coordinate with the PDU for power distribution.\n",
    "\n",
    "4. Communication Infrastructure\n",
    "Primary TCP/IP networks connect through LAN and WAN interfaces. The Ethernet LAN provides \n",
    "local connectivity, supported by RS485 and RS 422 serial connections. Each Control System \n",
    "interfaces with its respective Monitoring Unit through dedicated channels.\n",
    "\n",
    "5. Monitoring and Control\n",
    "The CCTV System provides visual monitoring capabilities, integrated with the Control System \n",
    "and Monitoring Equipment. Outstation Equipment handles remote operations, while the \n",
    "Instation Interface manages central control functions.\n",
    "\n",
    "6. Reference Documentation\n",
    "MCE 1080B describes the detailed protocols, while TR 2044 and MCH 1075B provide \n",
    "supplementary specifications. The Requirements Document and Technical Requirements \n",
    "specify additional integration parameters.\n",
    "\n",
    "7. System Components\n",
    "Multiple AMI-EE installations connect through Cabinet Type 600B units, monitored by \n",
    "the Signal Subsystem. The MIDAS Gold deployment utilizes standard NMCS2 protocols for \n",
    "primary operations.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5704cd-6bb3-4b15-b4fa-6e97c1e3dd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "custom_labels = ['TECH_DOC', 'SYSTEM_COMPONENT', 'HARDWARE_COMPONENT', 'COMMUNICATION_COMPONENT', 'SUBSYSTEM_COMPONENT', 'CONTROL_COMPONENT', 'SPECIFICATION_TYPE']\n",
    "\n",
    "\n",
    "def process_text(nlp, text: str):\n",
    "    \"\"\"\n",
    "    Process text and return detailed entity information including:\n",
    "    - Individual entity frequencies\n",
    "    - Entity type counts\n",
    "    - Context and position information\n",
    "    \n",
    "    This enhanced tracking helps build a more informed knowledge graph by showing\n",
    "    which specific entities are most referenced in the documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # tracking dictionaries\n",
    "    entity_type_counts = {label: 0 for label in custom_labels}  # Counts by entity type\n",
    "    entity_frequencies = {}  # Counts of specific entity mentions\n",
    "    \n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in custom_labels:\n",
    "            # Create entity record\n",
    "            entity_info = {\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'original_text': text[ent.start_char:ent.end_char],\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char\n",
    "            }\n",
    "            entities.append(entity_info)\n",
    "            \n",
    "            entity_type_counts[ent.label_] += 1\n",
    "            \n",
    "            # Update specific entity frequency\n",
    "            entity_key = (ent.text, ent.label_)  # Tuple of text and label to handle same text with different labels\n",
    "            if entity_key not in entity_frequencies:\n",
    "                entity_frequencies[entity_key] = {\n",
    "                    'count': 0,\n",
    "                    'text': ent.text,\n",
    "                    'label': ent.label_\n",
    "                }\n",
    "            entity_frequencies[entity_key]['count'] += 1\n",
    "    \n",
    "    return entities, entity_type_counts, entity_frequencies\n",
    "\n",
    "def print_document_results(entities, type_counts, frequencies):\n",
    "    \"\"\"\n",
    "    Display comprehensive entity analysis including:\n",
    "    - Individual entities found\n",
    "    - Counts by entity type\n",
    "    - Frequency of specific entities\n",
    "    \"\"\"\n",
    "    print(\"\\nDocument Processing Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"Entities Found in Context:\")\n",
    "    for entity in entities:\n",
    "        print(f\"Found: {entity['text']} ({entity['label']})\")\n",
    "        print(f\"Original text: '{entity['original_text']}'\")\n",
    "        print(f\"Position: {entity['start']} to {entity['end']}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nEntity Type Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    for label, count in type_counts.items():\n",
    "        if count > 0:  \n",
    "            print(f\"{label}: {count} total mentions\")\n",
    "    \n",
    "    print(\"\\nDetailed Entity Frequencies:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Group frequencies by entity type for clearer presentation\n",
    "    grouped_frequencies = {}\n",
    "    for (text, label), info in frequencies.items():\n",
    "        if label not in grouped_frequencies:\n",
    "            grouped_frequencies[label] = []\n",
    "        grouped_frequencies[label].append(info)\n",
    "    \n",
    "    # Print frequencies by type\n",
    "    for label in custom_labels:\n",
    "        if label in grouped_frequencies:\n",
    "            print(f\"\\n{label}:\")\n",
    "            # Sort by frequency, highest first\n",
    "            sorted_entities = sorted(grouped_frequencies[label], \n",
    "                                  key=lambda x: x['count'], \n",
    "                                  reverse=True)\n",
    "            for entity in sorted_entities:\n",
    "                print(f\"  {entity['text']}: {entity['count']} mentions\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "360af0af-951e-4a08-819a-ba08876fe22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the NLP pipeline\n",
    "nlp = create_nlp_pipeline()\n",
    "\n",
    "\n",
    "debug_document_code(nlp, \"MCE0107B , MCE2344B x  TR 0543 C more, MCE 2343: bro nn, MCE 3342C cap, MCE 3333 B bag, MCE3234 A stir tr 2144 m:3952 test this he Lane SAC Priority Data section contains the priority table for lane SAC settings (see TR 2163\\I:410). \") \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee6d8312-2589-4fd0-ab72-804008ff8c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s_chunk = str(small_chunk)\n",
    " \n",
    "entities, counts, freq = process_text(nlp, s_chunk)\n",
    "print_document_results(entities, counts, freq)\n",
    "\n",
    "debug_document_code(nlp, s_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e10f7e29-103e-436e-aee7-9da2fc7567b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "big_chunk = str(main_text)\n",
    " \n",
    "entities, counts, freq = process_text(nlp, big_chunk)\n",
    "print_document_results(entities, counts, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ffbe915-4dde-48a2-8650-4bfeb61dd766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(big_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5050e3-ad91-4427-bccb-d6e51d503f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "manual_counts1 = {'TR 1100': 10, 'TR 2070': 8, 'TR 2142': 4, 'TR 2043': 3, 'TR 2067': 4, 'TR 2130': 3, 'TR2070': 3, 'TR 2072': 2, 'TR2139': 1, 'MCE 1349': 3, 'MCE0110': 1, 'MCE0107': 3, 'MCH 1618': 2, 'MCX 0731': 1, 'MCX 0925': 1, 'MCX 0910': 1, 'TRH 1679':1, 'TRH 1680':4, 'TRG 0500':1}\n",
    "\n",
    "\n",
    "manual_counts2 = {\n",
    "        'TR 2033': 5,'TR 2043': 14,'TR 1100': 12,'TR 2070': 6,'TR 2130': 5,'TR 2142': 5,'TR 2067': 3,'TR2070': 4,'TR 2072': 2,'TR2139': 1,'TR 1173': 1,'TR 1238': 1,'TR 2110': 1,'MCX1031':17,'MCX0920': 3,'MCX0918': 1,'MCX0733': 1,'MCH 1618': 2,'MCH 1689': 2,'MCH1349': 1,'MCH 1621': 1,'MCE 0110': 1,'MCE0110': 2,'MCE0107': 1,'MCG 1069': 1,'TRH 1679':2, 'TRG 0500':2}\n",
    "\n",
    "manual_counts3 = {'mce 1157': 2, 'mce 1157 a': 1, 'mce 1157 b': 1, 'mce 1157 c': 1, 'mce 1157 d': 1, 'mce 1157 e': 1, 'tr 1100': 7, 'mcx 0708': 4, 'tr 2130': 4, 'mch 1616': 3, 'mch 1618': 3, 'tr 2199': 3, 'mcg 1107': 2, 'mch 1349': 2, 'tr2199': 1}\n",
    "\n",
    "manual_counts4 = {'mch 1744': 23, 'tr 2144': 11, 'mcg 1091': 3, 'tr2144': 3, 'mcg 1069': 3, 'mcg 1092': 2, 'mcg 1093': 1, 'mch 1714': 1, 'mch 1753': 1, 'mch 1748': 1}\n",
    "\n",
    "manual_counts5 = {'tr 2199': 138, 'tr 2130': 16, 'tr 1100': 14, 'tr 2067': 13, 'tr 2070': 10, 'mcg 1069': 5, 'tr 2516': 5, 'mce 1137': 4, 'mch 1689': 3, 'mcx 0028': 3, 'tr 2195': 2, 'mcx 0071': 2, 'tr 2045': 2, 'mcg 1107': 1, 'mch 1616': 1}\n",
    "\n",
    "manual_counts6 = {'mcg 1069': 3, 'mce 0110': 3, 'tr 1100': 2, 'tr 2199': 2, 'mce 0107': 2, 'tr 2195': 2, 'mce 2214': 2, 'mch 1616': 2, 'mcg 1202': 1}\n",
    "\n",
    "manual_counts7 = {'mch 1753': 13, 'tr 2144': 4, 'mch 1744': 1}\n",
    "\n",
    "manual_counts8 = {'mch 1748': 39, 'tr 2163': 23, 'tr 2133': 5, 'mch 1726': 4, 'tr 2139': 3, 'mch 1689': 3, 'mch 1617': 2, 'mch 1618': 2, 'mch 1655': 2, 'tr 2072': 1, 'mch 1700': 2, 'mch 1759': 1, 'mch1619': 1, 'mch 1124': 1}\n",
    "\n",
    "manual_counts9 = {'mch 1748': 38, 'mch 1689': 14, 'mce 2103': 4, 'mch 1700': 4, 'mch 1616': 3, 'tr 2072': 3, 'tr 2133': 2, 'mch 1798': 1, 'mch 1619': 1}\n",
    "\n",
    "manual_counts10 = {'mch2624': 4, 'mch1689': 2, 'mch 2629': 1}\n",
    "\n",
    "manual_counts11 = {\"mce 2240\": 14, \"mcg 1110\": 8, \"mce 2536\": 5, \"tr 1100\": 7, \"tr 2130\": 4, \"mce 1126\": 4, \"mce 2135\": 3, \"tr 2180\": 3, \"mce 1233\": 3, \"mce 0110\": 2, \"mch 1514\": 2, \"tr 2189\": 3, \"mch 1619\": 1}\n",
    "\n",
    "manual_counts12 = {\"mce 2242\": 8, \"mce 2245\": 6, \"mce 2240\": 4, \"mce2247\": 4, \"mce 2247\": 3, \"mce 2239\": 1, \"mce 2241\": 1, \"mce 2135\": 1, \"mce 2013\": 1, \"mce 2216\": 1, \"mce 1959\": 1, \"mch 1959\": 4, \"mch 1960\": 1, \"mch 1970\": 1, \"mcg 1110\": 1}\n",
    "\n",
    "manual_counts13 = {\"tr 2145\": 18, \"tr 2172\": 9, \"tr 2173\": 8, \"tr 1100 c\": 1, \"tr 2145 d\": 1, \"tr 2172 d\": 1, \"tr 2173 j\": 1, \"re 2177 g\": 1}\n",
    "\n",
    "manual_counts14 = {\"mch 1700\": 18, \"mch 1596\": 2, \"mcg 1069\": 7, \"mce 2103\": 5, \"mcg 1075\": 3, \"mcg 1086\": 3, \"mcg 1091\": 3, \"mcg 1077\": 2}\n",
    "\n",
    "manual_counts18 = {\"mch 1781\": 2, \"mce2242\": 3, \"mcg 1110\": 1, \"mch 1960\": 1, \"mch 1970\": 1, \"mch 1959\": 1, \"mch 1731\": 1, \"mce 2239\": 1, \"mce 2240\": 1, \"mce 2241\": 1, \"mce 2216\": 1, \"mce 2247\": 1, \"mce 2242\": 1, \"mce 2246\": 1}\n",
    "\n",
    "\n",
    "manual_counts19={'mch 1760':9, 'mch1865':6, 'mch 1952':3, 'mch 1951':3, 'mch 1957':3, 'mch 1696':2, 'mch 1867':2, 'mch 1857':1,}\n",
    "\n",
    "manual_counts20 = {\"mch 2470\": 6, \"mch1965\": 5, \"mch 2474\": 4, \"mch 2472\": 3, \"mch 2473\": 3, \"mch1349\": 3, \"mch2471\": 2, \"mch1514\": 2, \"mch1144\": 2, \"mch1147\": 2, \"mch1148\": 2, \"mch 1965\": 1, \"mch2472\": 1, \"mch 2471\": 1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4d3bdf0b-3047-432a-9920-e5e9d7689725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "copilot_count1 = {'MCE0110': 1,'MCH1618': 2,'MCE0107': 3,'TR2043': 3,'TR2067': 4,'TR2070': 11,'TR2072': 2,'TR1100': 9,'MCE1349': 3,'TR2130': 3,'MCX0731': 1,'MCX0925': 1,'TR2033': 2,'MCX0910': 1,'TR2142': 4,'MCG1069': 1,'TR2139': 1\n",
    "}\n",
    "\n",
    "copilot_count2 = {'MCE0107': 1, 'MCE0110': 3, 'MCH1618': 2, 'MCX1031': 15, 'TR2043': 13, 'MCH1689': 2, 'TR2070': 9, 'MCX0920': 3, 'TR1100': 12, 'MCH1349': 1, 'TR2130': 4, 'TR2067': 3, 'MCX0918': 1, 'TR2033': 4, 'MCX0733': 1, 'TR2142': 5, 'MCG1069': 1, 'TR2072': 2, 'TR2139': 1, 'MCH1621': 1, 'TR1173': 1, 'TR1238': 1, 'TR2110': 1, 'MCE 0107 B': 1, 'MCX 1031': 2}\n",
    "\n",
    "copilot_count3 = {'MCE1157': 7, 'MCH1616': 3, 'MCH1618': 3, 'TR2199': 4, 'MCG1107': 2, 'TR1100': 7, 'MCX0708': 4, 'MCH1349': 2, 'TR2130': 4}\n",
    "\n",
    "copilot_count4 = {'MCG1093': 1, 'TR2144': 14, 'MCH1744': 23, 'MCG1069': 3, 'MCG1091': 3, 'MCG1092': 2, 'MCH1714': 1, 'MCH1753': 1, 'MCH1748': 1}\n",
    "\n",
    "copilot_count6 = {'MCG1202': 1, 'TR1100': 2, 'MCG1069': 3, 'TR2199': 2, 'MCE0107': 2, 'TR2195': 2, 'MCE2214': 2, 'MCE0110': 3, 'MCH1616': 2}\n",
    "\n",
    "copilot_count7 = {'MCH1744': 1, 'TR2144': 4, 'MCH1753': 11}\n",
    "\n",
    "copilot_count8= {'MCH1759': 1, 'MCH1726': 4, 'MCH1617': 2, 'MCH1618': 2, 'MCH1655': 2, 'TR2139': 3, 'TR2163': 23, 'MCH1748': 39, 'MCH1689': 3, 'TR2133': 5, 'MCH1700': 2, 'TR2072': 1, 'MCH1619': 1, 'MCH1124': 1}\n",
    "\n",
    "copilot_count9 = {'MCH1798': 1, 'MCH1616': 3, 'MCH1748': 38, 'MCE2103': 4, 'MCH1689': 14, 'TR2072': 3, 'MCH1700': 4, 'TR2133': 1, 'MCH1619': 1}\n",
    "\n",
    "copilot_count10 = {'MCH2629': 1, 'MCH2624': 4, 'MCH1689': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af8fcb8-7d40-49f4-af8b-64975d099e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracies_for_all_pdfs(nlp):\n",
    "    pdf_accuracies = {}\n",
    "    \n",
    "    # List of all files and their corresponding manual counts\n",
    "    files_man = [\n",
    "        (file1, manual_counts1, \"MCE0110B\"),\n",
    "        (file2, manual_counts2, \"MCE0107B\"),\n",
    "        (file3, manual_counts3, \"MCE1157E\"),\n",
    "        (file4, manual_counts4, \"MCG1093J\"),\n",
    "        (file5, manual_counts5, \"MCG1107B\"),\n",
    "        (file6, manual_counts6, \"MCG1202A\"),\n",
    "        (file7, manual_counts7, \"MCH1744H\"),\n",
    "        (file8, manual_counts8, \"MCH1759F\"),\n",
    "        (file9, manual_counts9, \"MCH1798H\"),\n",
    "        (file10, manual_counts10, \"MCH2629A\")\n",
    "    ]\n",
    "\n",
    "    files_man2 = [\n",
    "    (file11, manual_counts11, \"MCE2241F\"),\n",
    "    (file12, manual_counts12, \"MCE2246A\"),\n",
    "    (file13, manual_counts13, \"MCG1090D\"),\n",
    "    (file14, manual_counts14, \"MCG1094C\"),\n",
    "    (file18, manual_counts18, \"MCH1734A\"),\n",
    "    (file19, manual_counts19, \"MCH1948B\"),\n",
    "    (file20, manual_counts20, \"MCH2475C\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    files_copilot = [\n",
    "        (file1, copilot_count1, \"MCE0110B\"),\n",
    "        (file2, copilot_count2, \"MCE0107B\"),\n",
    "        (file3, copilot_count3, \"MCE1157E\"),\n",
    "        (file4, copilot_count4, \"MCG1093J\"),\n",
    "        (file6, copilot_count6, \"MCG1202A\"),\n",
    "        (file7, copilot_count7, \"MCH1744H\"),\n",
    "        (file8, copilot_count8, \"MCH1759F\"),\n",
    "        (file9, copilot_count9, \"MCH1798H\"),\n",
    "        (file10, copilot_count10, \"MCH2629A\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    \n",
    "    for file_path, manual_counts, pdf_name in files_man:\n",
    "        # Read PDF content\n",
    "        pdf_df = spark.read.format(\"binaryFile\").load(file_path).cache()\n",
    "        binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"]\n",
    "        \n",
    "        # Extract text\n",
    "        extracted_text = extract_text(binary_pdf)\n",
    "        \n",
    "        text_for_validation = process_pdf_text(extracted_text)\n",
    "        \n",
    "        # Run validation\n",
    "        _, accuracy = validate_tech_doc_recognition(nlp, text_for_validation, manual_counts)\n",
    "        pdf_accuracies[pdf_name] = accuracy\n",
    "        \n",
    "        print(f\"{pdf_name} Accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        # Clear cache\n",
    "        pdf_df.unpersist()\n",
    "    \n",
    "    return pdf_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc393e5a-6254-452f-9118-995e66940c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from typing import Dict, List, Tuple\n",
    "from builtins import min, abs\n",
    "\n",
    "\n",
    "def normalize_tech_doc_id(doc_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes technical document IDs by removing spaces and converting to lowercase.\n",
    "    For example: 'MCE 0107 B' -> 'mce0107b'\n",
    "    \"\"\"\n",
    "    return ''.join(doc_id.split()).lower()\n",
    "\n",
    "def validate_tech_doc_recognition(nlp, text: str, manual_counts: Dict[str, int]) -> Tuple[PrettyTable, float]:\n",
    "    \"\"\"\n",
    "    Validates the NER model's performance on technical document recognition,\n",
    "    treating different format variations of the same document ID as equivalent.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # create normalized versions of manual counts and track variations\n",
    "    normalized_manual_counts = {}\n",
    "    variations_map = {}  # Maps normalized IDs to sets of original variations\n",
    "    \n",
    "    for original_id, count in manual_counts.items():\n",
    "        normalized_id = normalize_tech_doc_id(original_id)\n",
    "        \n",
    "        # Update normalized counts\n",
    "        if normalized_id not in normalized_manual_counts:\n",
    "            normalized_manual_counts[normalized_id] = 0\n",
    "            variations_map[normalized_id] = set()\n",
    "        \n",
    "        normalized_manual_counts[normalized_id] += count\n",
    "        variations_map[normalized_id].add(original_id)\n",
    "    \n",
    "    # Count model predictions, normalizing as we go\n",
    "    predicted_counts = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"TECH_DOC\":\n",
    "            normalized_ent = normalize_tech_doc_id(ent.text)\n",
    "            if normalized_ent not in predicted_counts:\n",
    "                predicted_counts[normalized_ent] = 0\n",
    "            predicted_counts[normalized_ent] += 1\n",
    "            \n",
    "            # Add this variation to our tracking if it's a new format\n",
    "            if normalized_ent in variations_map:\n",
    "                variations_map[normalized_ent].add(ent.text)\n",
    "    \n",
    "    # Create comparison table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\n",
    "        \"Technical Document\",\n",
    "        \"Variations Found\",\n",
    "        \"Manual Count\",\n",
    "        \"Model Count\",\n",
    "        \"Difference\",\n",
    "        \"Accuracy %\"\n",
    "    ]\n",
    "    table.align = \"l\"\n",
    "    \n",
    "    # Track totals\n",
    "    total_manual = 0\n",
    "    total_predicted = 0\n",
    "    total_correct = 0\n",
    "    total_difference = 0\n",
    "    \n",
    "    # Add rows for each unique normalized document ID\n",
    "    processed_ids = set()\n",
    "    \n",
    "    # First, process all manual counts\n",
    "    for normalized_id in normalized_manual_counts.keys():\n",
    "        if normalized_id in processed_ids:\n",
    "            continue\n",
    "            \n",
    "        processed_ids.add(normalized_id)\n",
    "        \n",
    "        manual_count = normalized_manual_counts[normalized_id]\n",
    "        predicted_count = predicted_counts.get(normalized_id, 0)\n",
    "        \n",
    "        # Get all variations found\n",
    "        variations = sorted(variations_map[normalized_id])\n",
    "        variations_str = \", \".join(variations)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = 100 - (abs(predicted_count - manual_count) / manual_count * 100) if manual_count > 0 else 0       \n",
    "\n",
    "        # Update totals\n",
    "        total_manual += manual_count\n",
    "        total_predicted += predicted_count\n",
    "        total_correct += min(predicted_count, manual_count)\n",
    "        total_difference += abs(predicted_count - manual_count)\n",
    "        \n",
    "        # Use the first variation as the primary ID for display\n",
    "        primary_id = sorted(variations_map[normalized_id])[0]\n",
    "        \n",
    "        table.add_row([\n",
    "            primary_id,\n",
    "            variations_str,\n",
    "            manual_count,\n",
    "            predicted_count,\n",
    "            predicted_count - manual_count,\n",
    "            f\"{accuracy:.1f}%\"\n",
    "        ])\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = 100 - (total_difference / total_manual * 100) if total_manual > 0 else 0\n",
    "    \n",
    "    # Add totals row\n",
    "    table.add_row([\n",
    "        \"TOTAL\",\n",
    "        \"\",\n",
    "        total_manual,\n",
    "        total_predicted,\n",
    "        total_predicted - total_manual,\n",
    "        f\"{overall_accuracy:.1f}%\"\n",
    "    ])\n",
    "    \n",
    "    return table, float(overall_accuracy)\n",
    "\n",
    "# Run validation\n",
    "results_table, overall_accuracy = validate_tech_doc_recognition(nlp, big_chunk, manual_counts2)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80abfa3-6829-498b-a9c4-bffd74e2ca18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracies):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    pdfs = list(accuracies.keys())\n",
    "    acc_values = list(accuracies.values())\n",
    "    \n",
    "    bars = plt.bar(pdfs, acc_values, color='skyblue')\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Technical Document Recognition Accuracy Across PDFs', pad=20)\n",
    "    plt.xlabel('PDF Documents')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set y-axis range\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Add average line\n",
    "    avg_accuracy = np.mean(acc_values)\n",
    "    plt.axhline(y=avg_accuracy, color='r', linestyle='--', alpha=0.8)\n",
    "    plt.text(len(pdfs)-1, avg_accuracy, f'Average: {avg_accuracy:.1f}%', \n",
    "             va='bottom', ha='right', color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ffcdda3-c907-4580-9a43-1adac253f088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def main():\n",
    "    nlp = create_nlp_pipeline()\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTechnical Document Recognition Validation\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_table)\n",
    "    print(f\"\\nOverall Model Accuracy: {overall_accuracy:.1f}%\")\n",
    "\n",
    "    # Get accuracies for all PDFs\n",
    "    accuracies = get_accuracies_for_all_pdfs(nlp)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_accuracies(accuracies)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "PDF LLM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
