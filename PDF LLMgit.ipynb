{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c8bf4ebd-2cfe-4e7a-8306-d7cb04480fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba5dbd2-7a69-4326-82fc-e0bc66d162e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73c535c-d53d-42e5-aabb-18c5d7b4ea7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0b2694-f8cb-4ec0-82f3-892b8b0d6aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Azure Blob Storage connection details\n",
    "storage_account_name = \"heauksdsdevcdosa\"\n",
    "storage_account_access_key = dbutils.secrets.get(scope=\"cdo-kv\", key=\"heauksdsdevcdosa-key\")\n",
    "container_name = 'lb-container'\n",
    "\n",
    "# Initialize the Azure Blob Service Client\n",
    "block_blob_service = BlobServiceClient(account_url=f\"https://{storage_account_name}.blob.core.windows.net\", credential= storage_account_access_key)\n",
    "\n",
    "# Set up Spark to authenticate with the Blob Storage\n",
    "spark.conf.set(f'fs.azure.account.key.{storage_account_name}.blob.core.windows.net',storage_account_access_key)\n",
    "\n",
    "# Define the path to the PDF file in the Blob Storage\n",
    "file1 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCE0110B.pdf\"\n",
    "file2 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCE0107B.pdf\"\n",
    "file3 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCE1157E.pdf\"\n",
    "file4 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCG1093J.pdf\"\n",
    "file5 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCG1107B.pdf\"\n",
    "file6 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCG1202A.pdf\"\n",
    "file7 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCH1744H.pdf\"\n",
    "file8 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCH1759F.pdf\"\n",
    "file9 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCH1798H.pdf\"\n",
    "file10 = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/MCH2629A.pdf\"\n",
    "\n",
    "\n",
    "# Read the binary content of the file into a Spark DataFrame and then display\n",
    "pdf_df = spark.read.format(\"binaryFile\").load(file4).cache()\n",
    "display(pdf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c042e4a6-3fe2-4c73-bf66-a9171feb5e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pdfplumber\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "%pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63310f68-2491-4499-b74a-1e67f85acfa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_text(binary_data):\n",
    "  # wrap the binary data in a file-like object\n",
    "  pdf_file = BytesIO(binary_data)\n",
    "\n",
    "  # loading binary data into PDFplumber\n",
    "  with pdfplumber.open(pdf_file) as pdf:\n",
    "    plain_text = \"\"\n",
    "\n",
    "    for page in pdf.pages:\n",
    "      plain_text += page.extract_text()\n",
    "\n",
    "    return plain_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d455e56-8bff-4284-9da6-325e158834a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"] #collects rows from the dataframe into a python list\n",
    "\n",
    "extracted_text = extract_text(binary_pdf) \n",
    "#print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28997c35-c36a-4678-8ad5-954d2f239c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "# cleAns dataset\n",
    "    cleaned_text = \" \".join(text.split())\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_text = clean_text(extracted_text)\n",
    "print(cleaned_text[:1000])\n",
    "\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "#https://www.youtube.com/watch?v=dXxQ0LR-3Hg&list=LL&index=1&t=1820s to explain the function\n",
    "\n",
    "def get_text_chunks(text):\n",
    "  text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    separator=\"/n\", \n",
    "    length_function=len\n",
    "    )\n",
    "  chunks = text_splitter.split_text(text)\n",
    "  return chunks\n",
    "\n",
    "chunked_text = get_text_chunks(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a2a27c-3bcb-487e-b0db-ca76fbe69a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tests = \"\"\"\n",
    "The TR 2144 M:3952 document specifies requirements.\n",
    "Also see MCE 1234:123 and TR 2144 M for details.\n",
    "Some MCE1234B document and TR 2144M reference.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(clean_text(tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4d8a89-ba7a-4248-bde8-1cde44ca3542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''USEFUL LINKS\n",
    "https://www.babelstreet.com/blog/what-is-entity-extraction#:~:text=Entity%20extraction%20(aka%2C%20named%20entity,%2C%20webpages%2C%20text%20fields).\n",
    "\n",
    "https://medium.com/@sanskrutikhedkar09/mastering-information-extraction-from-unstructured-text-a-deep-dive-into-named-entity-recognition-4aa2f664a453\n",
    "\n",
    "https://www.microfocus.com/documentation/relativity/relativity1217/reldbdsn/GUID-7C2DF185-41A1-4448-81E7-3252AA8DEBB3.html \n",
    "\n",
    "'''\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def create_nlp_pipeline():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    \n",
    "    patterns = [\n",
    "    # Pattern 1: Connected with optional letter (mce0107b or mce0107)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(mce|mch|mcx|mcg|tr)\\d{4}[a-z]?$\"}}\n",
    "    ]},\n",
    "\n",
    "    # Pattern 2: Space after prefix (MCE 0107B or MCE 0107)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        # Match the prefix more flexibly\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\"]}},\n",
    "        # Match any numbers with optional suffix, removing strict boundaries\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}[A-Za-z]?$\"}}\n",
    "    ]},\n",
    "\n",
    "    # Pattern 3: Prefix, number, and separate letter (mce 0107 b)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\",\"tr\"]}},\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}$\"}},\n",
    "        {\"LOWER\": {\"REGEX\": r\"^[a-z]$\"}},\n",
    "        \n",
    "    ]},\n",
    "    \n",
    "    # SYSTEM_COMPONENT Patterns\n",
    "    {\"label\": \"SYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(midas|nmcs2?|hadecs|hatms)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"midas\"}, \n",
    "        {\"LOWER\": \"gold\"}\n",
    "    ]},\n",
    "    \n",
    "    # HARDWARE_COMPONENT Patterns\n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(cabinet|plinth|lantern|post|frame|skirt)$\"}}, \n",
    "        {\"LOWER\": \"type\"}, \n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d+[a-z]?$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(ms[1-4]r?|ami|ert)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"indicator\", \"signal\", \"sensor\", \"detector\", \"camera\", \"telephone\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # COMMUNICATION_COMPONENT Patterns\n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(rs485|rs422|tcp\\/ip|lan|wan)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"rs\"}, \n",
    "        {\"TEXT\": {\"REGEX\": r\"^(485|422)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"ethernet\"}, \n",
    "        {\"LOWER\": {\"IN\": [\"lan\", \"connection\", \"interface\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # SUBSYSTEM_COMPONENT Patterns\n",
    "    {\"label\": \"SUBSYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"signal\", \"message\", \"meteorological\", \"tidal\", \"tunnel\"]}}, \n",
    "        {\"LOWER\": \"subsystem\"}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SUBSYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(lcc|pdu|cobs|ceclb|ceceb|cecr)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    # CONTROL_COMPONENT Patterns\n",
    "    {\"label\": \"CONTROL_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"control\", \"monitoring\", \"outstation\", \"instation\"]}}, \n",
    "        {\"LOWER\": {\"IN\": [\"system\", \"unit\", \"equipment\", \"interface\"]}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"CONTROL_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"cctv\"}, \n",
    "        {\"LOWER\": {\"IN\": [\"system\", \"camera\", \"equipment\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # SPECIFICATION_TYPE Patterns\n",
    "    {\"label\": \"SPECIFICATION_TYPE\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"requirements\", \"specification\", \"instructions\", \"overview\", \"process\"]}}, \n",
    "        {\"LOWER\": \"document\"}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SPECIFICATION_TYPE\", \"pattern\": [\n",
    "        {\"LOWER\": \"technical\"}, \n",
    "        {\"LOWER\": \"requirements\"}\n",
    "    ]}\n",
    "]\n",
    "    \n",
    "    ruler.add_patterns(patterns)\n",
    "    return nlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a653db-fdd4-4727-b9d8-5064e4c684eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def debug_document_code(nlp, text):\n",
    "    \"\"\"\n",
    "    Provides detailed analysis of how document codes are being processed.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing document code: '{text}'\")\n",
    "    \n",
    "    # First, show raw tokenization\n",
    "    doc = nlp(text)\n",
    "    print(\"\\nTokenization details:\")\n",
    "    for token in doc:\n",
    "        print(f\"Token: '{token.text}'\")\n",
    "        print(f\"  Position: {token.idx} to {token.idx + len(token.text)}\")\n",
    "        print(f\"  Is part of entity: {token.ent_type_ != ''}\")\n",
    "        print(f\"  Entity type: {token.ent_type_ if token.ent_type_ else 'None'}\")\n",
    "        print()\n",
    "    \n",
    "    # Show complete entities found\n",
    "    print(\"\\nComplete entities found:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Entity: '{ent.text}'\")\n",
    "        print(f\"  Label: {ent.label_}\")\n",
    "        print(f\"  Includes all tokens: {all(t.ent_type_ == ent.label_ for t in ent)}\")\n",
    "        print()\n",
    "    \n",
    "    # Show what didn't match\n",
    "    unmatched = [t.text for t in doc if not t.ent_type_]\n",
    "    if unmatched:\n",
    "        print(\"\\nUnmatched tokens:\")\n",
    "        print(\", \".join(unmatched))\n",
    "     \n",
    "\n",
    "def test_pattern_variations(nlp) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Tests various pattern matching scenarios to verify entity recognition.\n",
    "    Includes comprehensive test cases for all entity types and their variations.\n",
    "    \"\"\"\n",
    "    # We organize test cases by category for better clarity and coverage\n",
    "    test_cases = [\n",
    "    # === Technical Document Tests ===\n",
    "    # Positive Cases\n",
    "    \"MCE 0107B\",             # Traditional uppercase format\n",
    "    \"mce0107b\",             # Complete lowercase\n",
    "    \"MCE-0107B\",            # With hyphen uppercase\n",
    "    \"MCE 0107 B\",           # Spaced format\n",
    "    \n",
    "    # Negative Cases\n",
    "    \"MCE01007B\",           # Too many digits\n",
    "    \"MCE107B\",             # Too few digits\n",
    "    \"MCEA0107B\",           #  Extra character in prefix\n",
    "    \"MCE0107BC\",           #  Multiple suffix letters\n",
    "    \"MC-0107B\",            # Wrong prefix\n",
    "    \"MCE/0107B\",           # Wrong separator\n",
    "    \"MCE_0107_B\",          #  Underscore separator\n",
    "    \"MCE.0107.B\",          #  Dot separator\n",
    "    \n",
    "    # === System Component Tests ===\n",
    "    # Positive Cases\n",
    "    \"MIDAS\",               #Basic system name\n",
    "    \"NMCS2\",               #With number\n",
    "    \"MIDAS Gold\",          #Multi-word system\n",
    "    \n",
    "    # Negative Cases\n",
    "    \"MIDAS3\",              # Unexpected number\n",
    "    \"MIDAS-Gold\",          #Hyphenated format\n",
    "    \"MIDASGold\",           #Run together\n",
    "    \"MIDAS_GOLD\",          #  Underscore format\n",
    "    \n",
    "    # === Hardware Component Tests ===\n",
    "    # Positive Cases\n",
    "    \"Cabinet Type 600\",     # Standard cabinet\n",
    "    \"Plinth Type 1A\",      #  With letter suffix\n",
    "    \"MS3R\",                # Signal type\n",
    "    \n",
    "    # Negative Cases\n",
    "    \"Cabinet Type ABC\",     # : Non-numeric type\n",
    "    \"Cabinet Types 600\",    # Wrong word\n",
    "    \"MS5R\",                # Wrong number\n",
    "    \"MS3RR\",               #Extra letter\n",
    "    \n",
    "    # === Communication Component Tests ===\n",
    "    # Positive Cases\n",
    "    \"RS485\",               # No space format\n",
    "    \"RS 485\",              # Spaced format\n",
    "    \"Ethernet LAN\",        #  Multi-word\n",
    "    \n",
    "    # Negative Cases\n",
    "    \"RS-485\",              # Hyphenated\n",
    "    \"RS_485\",              #  Underscore\n",
    "    \"RS 4855\",             #  Wrong number\n",
    "    \"Ethernet_LAN\",        #  Wrong separator\n",
    "    \n",
    "    # Subsystem Component Tests  \n",
    "    # Positive Cases\n",
    "    \"Signal Subsystem\",     #  Standard format\n",
    "    \"LCC\",                  #   Short form\n",
    "    \n",
    "    # Negative Cases\n",
    "    \"Signal-Subsystem\",     #  Hyphenated\n",
    "    \"SignalSubsystem\",      #   Run together\n",
    "    \"LCCC\",                 #   Extra character\n",
    "    \n",
    "    #  Control Component Tests \n",
    "    # Positive Cases\n",
    "    \"Control System\",       #  : Basic format\n",
    "    \"CCTV System\",         #  With prefix\n",
    "    \n",
    "    # Negative Cases\n",
    "    \"Control-System\",       #  Hyphenated\n",
    "    \"ControlSystem\",        #   Run together\n",
    "    \"CCTV_System\",         #    Wrong separator\n",
    "    \n",
    "    # === Complex Test Cases ===\n",
    "    # Positive Cases\n",
    "    \"The MCE0107B document describes the MIDAS Gold system\",    # Valid: Multiple valid entities\n",
    "    \n",
    "    # Negative Cases\n",
    "    \"The MCE01007B document connects to RS-485\",               #  Multiple invalid formats\n",
    "    \"The MIDAS_Gold system uses Cabinet Type ABC\",             #   Multiple wrong separators\n",
    "    \"MCE0107BB is connected to RS_485 through CCTV_System\"     #   Multiple format errors\n",
    "]\n",
    "    \n",
    "    results = []\n",
    "    for test_text in test_cases:\n",
    "        doc = nlp(test_text)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        results.append({\n",
    "            'input': test_text,\n",
    "            'entities': entities,\n",
    "            'matched': len(entities) > 0,\n",
    "            'context': test_text\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "  \n",
    "\n",
    "def analyze_test_results(results):\n",
    "    \n",
    "    # Initialize groupings for analysis\n",
    "    entity_groups = {}\n",
    "    missed_matches = []\n",
    "    context_matches = []\n",
    "    \n",
    "    # Process results\n",
    "    for result in results:\n",
    "        # Group entities by type\n",
    "        for entity, label in result['entities']:\n",
    "            if label not in entity_groups:\n",
    "                entity_groups[label] = []\n",
    "            entity_groups[label].append({\n",
    "                'text': entity,\n",
    "                'context': result['context']\n",
    "            })\n",
    "            \n",
    "            # Track entities found in complex contexts\n",
    "            if len(result['context'].split()) > 3:  # More than 3 words indicates complex context\n",
    "                context_matches.append({\n",
    "                    'entity': entity,\n",
    "                    'label': label,\n",
    "                    'context': result['context']\n",
    "                })\n",
    "        \n",
    "        # Track potential missed matches\n",
    "        if not result['matched'] and len(result['input'].split()) <= 3:  # Simple cases that didn't match\n",
    "            missed_matches.append(result['input'])\n",
    "    \n",
    "    # Display comprehensive analysis\n",
    "    print(\"=== Pattern Matching Analysis ===\\n\")\n",
    "    \n",
    "    # Show matches by entity type\n",
    "    for label in sorted(entity_groups.keys()):\n",
    "        print(f\"\\n{label} Matches ({len(entity_groups[label])} total):\")\n",
    "        print(\"-\" * 60)\n",
    "        for match in entity_groups[label]:\n",
    "            print(f\"Entity: {match['text']}\")\n",
    "            print(f\"Context: {match['context']}\")\n",
    "            print()\n",
    "    \n",
    "    # Show context analysis\n",
    "    if context_matches:\n",
    "        print(\"\\n=== Complex Context Matches ===\")\n",
    "        print(\"-\" * 60)\n",
    "        for match in context_matches:\n",
    "            print(f\"Found {match['entity']} ({match['label']})\")\n",
    "            print(f\"In context: {match['context']}\")\n",
    "            print()\n",
    "    \n",
    "    # Show statistics\n",
    "    print(\"\\n=== Match Statistics ===\")\n",
    "    print(\"-\" * 60)\n",
    "    for label in sorted(entity_groups.keys()):\n",
    "        print(f\"{label}: {len(entity_groups[label])} matches\")\n",
    "    \n",
    "    # Show potential issues\n",
    "    if missed_matches:\n",
    "        print(\"\\n=== Potential Missed Matches ===\")\n",
    "        print(\"-\" * 60)\n",
    "        for text in missed_matches:\n",
    "            print(f\"No entities found in: {text}\")\n",
    "\n",
    "def run_pattern_tests(nlp):\n",
    "    \"\"\"\n",
    "    Executes pattern tests and provides comprehensive results analysis.\n",
    "    \"\"\"\n",
    "    print(\"Showing pattern matching analysis...\\n\")\n",
    "    \n",
    "    # Run the tests\n",
    "    results = test_pattern_variations(nlp)\n",
    "    \n",
    "    # Analyze and display results\n",
    "    analyze_test_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to print test results, with type hints for parameter and return value\n",
    "def print_test_results(results: List[Dict[str, Any]]):\n",
    "    # Print header for test results section\n",
    "    print(\"Pattern Matching Test Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Iterate through each test result\n",
    "    for result in results:\n",
    "        status = \"✓\" if result['matched'] else \"✗\"\n",
    "        \n",
    "        # Create formatted string of entities, or \"No match\" if none found\n",
    "        # Uses list comprehension to format each entity with its label\n",
    "        entities_str = ', '.join([f\"{ent[0]} ({ent[1]})\" for ent in result['entities']]) if result['entities'] else \"No match\"\n",
    "        \n",
    "        # Print formatted result line with consistent spacing\n",
    "        print(f\"{status} Input: {result['input']:<15} -> {entities_str}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee256f9-7b31-482e-a82f-517531652358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "small_chunk = \"\"\"\n",
    "The MCE0107B document connects to RS485 while MCH 1070B uses RS 422.\n",
    "MIDAS Gold system interfaces with the Ethernet LAN through Cabinet Type 600.\n",
    "The Signal Subsystem monitors the CCTV System and AMI-EE devices. (AMI bobo)\n",
    "\"\"\"\n",
    "\n",
    "medium_chunk = \"\"\"\n",
    "The assembly manual for MCH0107B specifies that RS485 components must be configured \n",
    "alongside RS422 adapters, with additional references to MCE 0150C outlined in \n",
    "Section 4.3 of the document. TR 2043 further details the integration with \n",
    "Ethernet LAN systems, enabling high-speed communication protocols compliant \n",
    "with IEEE 802.3 standards. The MIDAS Gold system connects through Cabinet Type 600A \n",
    "to the Control System, while monitoring occurs via the CCTV System and Signal Subsystem.\n",
    "\n",
    "The MCE 1070B requirements document describes interfacing with NMCS2 through standard \n",
    "protocols. Local connections use RS 485 for primary communication, supported by \n",
    "AMI-EE devices and monitored by the Outstation Equipment.\n",
    "\"\"\"\n",
    "\n",
    "large_chunk = \"\"\"\n",
    "Technical Requirements Document: System Integration Specification\n",
    "\n",
    "1. Overview\n",
    "The MCE0107B specification, in conjunction with MCH 1070B and TR 2043, defines the \n",
    "integration requirements for the MIDAS Gold system. Primary communication occurs through \n",
    "RS485 interfaces, while secondary protocols utilize RS 422 and Ethernet LAN connections.\n",
    "\n",
    "2. Hardware Components\n",
    "Cabinet Type 600 houses the main control units, with additional Cabinet Type 450A units \n",
    "for auxiliary systems. The AMI-EE devices interface with MS3R indicators and standard \n",
    "AMI units. Signal sensors and detector units provide environmental monitoring capabilities.\n",
    "\n",
    "3. System Architecture\n",
    "The NMCS2 framework integrates with HADECS and HATMS subsystems through standardized \n",
    "interfaces. The Signal Subsystem and Message Subsystem handle primary control operations, \n",
    "while the Meteorological Subsystem provides environmental data. CECLB and CECEB units \n",
    "coordinate with the PDU for power distribution.\n",
    "\n",
    "4. Communication Infrastructure\n",
    "Primary TCP/IP networks connect through LAN and WAN interfaces. The Ethernet LAN provides \n",
    "local connectivity, supported by RS485 and RS 422 serial connections. Each Control System \n",
    "interfaces with its respective Monitoring Unit through dedicated channels.\n",
    "\n",
    "5. Monitoring and Control\n",
    "The CCTV System provides visual monitoring capabilities, integrated with the Control System \n",
    "and Monitoring Equipment. Outstation Equipment handles remote operations, while the \n",
    "Instation Interface manages central control functions.\n",
    "\n",
    "6. Reference Documentation\n",
    "MCE 1080B describes the detailed protocols, while TR 2044 and MCH 1075B provide \n",
    "supplementary specifications. The Requirements Document and Technical Requirements \n",
    "specify additional integration parameters.\n",
    "\n",
    "7. System Components\n",
    "Multiple AMI-EE installations connect through Cabinet Type 600B units, monitored by \n",
    "the Signal Subsystem. The MIDAS Gold deployment utilizes standard NMCS2 protocols for \n",
    "primary operations.\n",
    "\"\"\"\n",
    "small_chunk= small_chunk.lower()\n",
    "medium_chunk = medium_chunk.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5704cd-6bb3-4b15-b4fa-6e97c1e3dd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#List of our cutom labels for the process text to look for\n",
    "custom_labels = ['TECH_DOC', 'SYSTEM_COMPONENT', 'HARDWARE_COMPONENT', 'COMMUNICATION_COMPONENT', 'SUBSYSTEM_COMPONENT', 'CONTROL_COMPONENT', 'SPECIFICATION_TYPE']\n",
    "\n",
    "\n",
    "def process_text(nlp, text: str):\n",
    "    \"\"\"\n",
    "    Process text and return detailed entity information including:\n",
    "    - Individual entity frequencies\n",
    "    - Entity type counts\n",
    "    - Context and position information\n",
    "    \n",
    "    This enhanced tracking helps build a more informed knowledge graph by showing\n",
    "    which specific entities are most referenced in the documentation.\n",
    "    \"\"\"\n",
    "    # Clean up whitespace while preserving document structure\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize tracking dictionaries\n",
    "    entity_type_counts = {label: 0 for label in custom_labels}  # Counts by entity type\n",
    "    entity_frequencies = {}  # Counts of specific entity mentions\n",
    "    \n",
    "    # Process entities and track frequencies\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in custom_labels:\n",
    "            # Create entity record\n",
    "            entity_info = {\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'original_text': text[ent.start_char:ent.end_char],\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char\n",
    "            }\n",
    "            entities.append(entity_info)\n",
    "            \n",
    "            # Update type count\n",
    "            entity_type_counts[ent.label_] += 1\n",
    "            \n",
    "            # Update specific entity frequency\n",
    "            entity_key = (ent.text, ent.label_)  # Tuple of text and label to handle same text with different labels\n",
    "            if entity_key not in entity_frequencies:\n",
    "                entity_frequencies[entity_key] = {\n",
    "                    'count': 0,\n",
    "                    'text': ent.text,\n",
    "                    'label': ent.label_\n",
    "                }\n",
    "            entity_frequencies[entity_key]['count'] += 1\n",
    "    \n",
    "    return entities, entity_type_counts, entity_frequencies\n",
    "\n",
    "def print_document_results(entities, type_counts, frequencies):\n",
    "    \"\"\"\n",
    "    Display comprehensive entity analysis including:\n",
    "    - Individual entities found\n",
    "    - Counts by entity type\n",
    "    - Frequency of specific entities\n",
    "    \"\"\"\n",
    "    print(\"\\nDocument Processing Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Print each found entity with context\n",
    "    print(\"Entities Found in Context:\")\n",
    "    for entity in entities:\n",
    "        print(f\"Found: {entity['text']} ({entity['label']})\")\n",
    "        print(f\"Original text: '{entity['original_text']}'\")\n",
    "        print(f\"Position: {entity['start']} to {entity['end']}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Print entity type summary\n",
    "    print(\"\\nEntity Type Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    for label, count in type_counts.items():\n",
    "        if count > 0:  # Only show types that were found\n",
    "            print(f\"{label}: {count} total mentions\")\n",
    "    \n",
    "    # Print specific entity frequencies, grouped by type\n",
    "    print(\"\\nDetailed Entity Frequencies:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Group frequencies by entity type for clearer presentation\n",
    "    grouped_frequencies = {}\n",
    "    for (text, label), info in frequencies.items():\n",
    "        if label not in grouped_frequencies:\n",
    "            grouped_frequencies[label] = []\n",
    "        grouped_frequencies[label].append(info)\n",
    "    \n",
    "    # Print frequencies by type\n",
    "    for label in custom_labels:\n",
    "        if label in grouped_frequencies:\n",
    "            print(f\"\\n{label}:\")\n",
    "            # Sort by frequency, highest first\n",
    "            sorted_entities = sorted(grouped_frequencies[label], \n",
    "                                  key=lambda x: x['count'], \n",
    "                                  reverse=True)\n",
    "            for entity in sorted_entities:\n",
    "                print(f\"  {entity['text']}: {entity['count']} mentions\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360af0af-951e-4a08-819a-ba08876fe22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the NLP pipeline\n",
    "nlp = create_nlp_pipeline()\n",
    "    \n",
    "    # Run pattern tests, run once\n",
    "    #test = run_pattern_tests(nlp)\n",
    "    #print_test_results(test)\n",
    "\n",
    "debug_document_code(nlp, \"MCE0107B , MCE2344B x  TR 0543 C more, MCE 234 bro nn, MCE 3342C cap, MCE 3333 B bag, MCE3234 A stir TR 2144 M:3952\") \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10f7e29-103e-436e-aee7-9da2fc7567b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "big_chunk = str(chunked_text)\n",
    " \n",
    " # Process the sample text\n",
    "entities, counts, freq = process_text(nlp, big_chunk)\n",
    "print_document_results(entities, counts, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5050e3-ad91-4427-bccb-d6e51d503f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "manual_counts1 = {'TR 1100': 10, 'TR 2070': 8, 'TR 2142': 4, 'TR 2043': 3, 'TR 2067': 4, 'TR 2130': 3, 'TR2070': 3, 'TR 2072': 2, 'TR2139': 1, 'MCE 1349': 3, 'MCE0110': 1, 'MCE0107': 3, 'MCH 1618': 2, 'MCX 0731': 1, 'MCX 0925': 1, 'MCX 1069': 1}\n",
    "\n",
    "\n",
    "manual_counts2 = {\n",
    "        'TR 2033': 5,'TR 2043': 14,'TR 1100': 12,'TR 2070': 6,'TR 2130': 5,'TR 2142': 5,'TR 2067': 3,'TR2070': 4,'TR 2072': 2,'TR2139': 1,'TR 1173': 1,'TR 1238': 1,'TR 2110': 1,'MCX1031':17,'MCX0920': 3,'MCX0918': 1,'MCX0733': 1,'MCH 1618': 2,'MCH 1689': 2,'MCH1349': 1,'MCH 1621': 1,'MCE 0110': 1,'MCE0110': 2,'MCE0107': 1,'MCG 1069': 1,\n",
    "    }\n",
    "\n",
    "manual_counts3 = {'mce 1157': 2, 'mce 1157 a': 1, 'mce 1157 b': 1, 'mce 1157 c': 1, 'mce 1157 d': 1, 'mce 1157 e': 1, 'tr 1100': 7, 'mcx 0708': 4, 'tr 2130': 4, 'mch 1616': 3, 'mch 1618': 3, 'tr 2199': 3, 'mcg 1107': 2, 'mch 1349': 2, 'tr2199': 1}\n",
    "\n",
    "manual_counts4 = {'mch 1744': 23, 'tr 2144': 11, 'mcg 1091': 3, 'tr2144': 3, 'tr 2144 m': 7, 'mcg 1069': 3, 'mcg 1092': 2, 'mcg 1093': 1, 'mch 1714': 1, 'mch 1753': 1, 'mch 1748': 1}\n",
    "\n",
    "manual_counts5 = {'tr 2199': 138, 'tr 2130': 16, 'tr 1100': 14, 'tr 2067': 13, 'tr 2070': 10, 'mcg 1069': 5, 'tr 2516': 5, 'mce 1137': 4, 'mch 1689': 3, 'mcx 0028': 3, 'tr 2195': 2, 'mcx 0071': 2, 'tr 2045': 2, 'mcg 1107': 1, 'mch 1616': 1}\n",
    "\n",
    "manual_counts6 = {'mcg 1069': 3, 'mce 0110': 3, 'tr 1100': 2, 'tr 2199': 2, 'mce 0107': 2, 'tr 2195': 2, 'mce 2214': 2, 'mch 1616': 2, 'mcg 1202': 1}\n",
    "\n",
    "manual_counts7 = {'mch 1753': 13, 'tr 2144': 4, 'mch 1744': 1}\n",
    "\n",
    "manual_counts8 = {'mch 1748': 39, 'tr 2163': 23, 'tr 2133': 5, 'mch 1726': 4, 'tr 2139': 3, 'mch 1689': 3, 'mch 1617': 2, 'mch 1618': 2, 'mch 1655': 2, 'tr 2072': 1, 'mch 1700': 2, 'mch 1759': 1, 'mch1619': 1, 'mch 1124': 1}\n",
    "\n",
    "manual_counts9 = {'mch 1748': 38, 'mch 1689': 14, 'mce 2103': 4, 'mch 1700': 4, 'mch 1616': 3, 'tr 2072': 3, 'tr 2133': 2, 'mch 1798': 1, 'mch 1619': 1}\n",
    "\n",
    "manual_counts10 = {'mch2624': 4, 'mch1689': 2, 'mch 2629': 1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3bdf0b-3047-432a-9920-e5e9d7689725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "copilot_count1 = {'MCE0110': 1,'MCH1618': 2,'MCE0107': 3,'TR2043': 3,'TR2067': 4,'TR2070': 11,'TR2072': 2,'TR1100': 9,'MCE1349': 3,'TR2130': 3,'MCX0731': 1,'MCX0925': 1,'TR2033': 2,'MCX0910': 1,'TR2142': 4,'MCG1069': 1,'TR2139': 1\n",
    "}\n",
    "\n",
    "copilot_count2 = {'MCE0107': 1, 'MCE0110': 3, 'MCH1618': 2, 'MCX1031': 15, 'TR2043': 13, 'MCH1689': 2, 'TR2070': 9, 'MCX0920': 3, 'TR1100': 12, 'MCH1349': 1, 'TR2130': 4, 'TR2067': 3, 'MCX0918': 1, 'TR2033': 4, 'MCX0733': 1, 'TR2142': 5, 'MCG1069': 1, 'TR2072': 2, 'TR2139': 1, 'MCH1621': 1, 'TR1173': 1, 'TR1238': 1, 'TR2110': 1, 'MCE 0107 B': 1, 'MCX 1031': 2}\n",
    "\n",
    "copilot_count3 = {'MCE1157': 7, 'MCH1616': 3, 'MCH1618': 3, 'TR2199': 4, 'MCG1107': 2, 'TR1100': 7, 'MCX0708': 4, 'MCH1349': 2, 'TR2130': 4}\n",
    "\n",
    "copilot_count4 = {'MCG1093': 1, 'TR2144': 14, 'MCH1744': 23, 'MCG1069': 3, 'MCG1091': 3, 'MCG1092': 2, 'MCH1714': 1, 'MCH1753': 1, 'MCH1748': 1}\n",
    "\n",
    "copilot_count6 = {'MCG1202': 1, 'TR1100': 2, 'MCG1069': 3, 'TR2199': 2, 'MCE0107': 2, 'TR2195': 2, 'MCE2214': 2, 'MCE0110': 3, 'MCH1616': 2}\n",
    "\n",
    "copilot_count7 = {'MCH1744': 1, 'TR2144': 4, 'MCH1753': 11}\n",
    "\n",
    "copilot_count8= {'MCH1759': 1, 'MCH1726': 4, 'MCH1617': 2, 'MCH1618': 2, 'MCH1655': 2, 'TR2139': 3, 'TR2163': 23, 'MCH1748': 39, 'MCH1689': 3, 'TR2133': 5, 'MCH1700': 2, 'TR2072': 1, 'MCH1619': 1, 'MCH1124': 1}\n",
    "\n",
    "copilot_count9 = {'MCH1798': 1, 'MCH1616': 3, 'MCH1748': 38, 'MCE2103': 4, 'MCH1689': 14, 'TR2072': 3, 'MCH1700': 4, 'TR2133': 1, 'MCH1619': 1}\n",
    "\n",
    "copilot_count10 = {'MCH2629': 1, 'MCH2624': 4, 'MCH1689': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af8fcb8-7d40-49f4-af8b-64975d099e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracies_for_all_pdfs(nlp):\n",
    "    pdf_accuracies = {}\n",
    "    \n",
    "    # List of all files and their corresponding manual counts\n",
    "    files_man = [\n",
    "        (file1, manual_counts1, \"MCE0110B\"),\n",
    "        (file2, manual_counts2, \"MCE0107B\"),\n",
    "        (file3, manual_counts3, \"MCE1157E\"),\n",
    "        (file4, manual_counts4, \"MCG1093J\"),\n",
    "        (file5, manual_counts5, \"MCG1107B\"),\n",
    "        (file6, manual_counts6, \"MCG1202A\"),\n",
    "        (file7, manual_counts7, \"MCH1744H\"),\n",
    "        (file8, manual_counts8, \"MCH1759F\"),\n",
    "        (file9, manual_counts9, \"MCH1798H\"),\n",
    "        (file10, manual_counts10, \"MCH2629A\")\n",
    "    ]\n",
    "\n",
    "    files_copilot = [\n",
    "        (file1, copilot_count1, \"MCE0110B\"),\n",
    "        (file2, copilot_count2, \"MCE0107B\"),\n",
    "        (file3, copilot_count3, \"MCE1157E\"),\n",
    "        (file4, copilot_count4, \"MCG1093J\"),\n",
    "        (file6, copilot_count6, \"MCG1202A\"),\n",
    "        (file7, copilot_count7, \"MCH1744H\"),\n",
    "        (file8, copilot_count8, \"MCH1759F\"),\n",
    "        (file9, copilot_count9, \"MCH1798H\"),\n",
    "        (file10, copilot_count10, \"MCH2629A\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    \n",
    "    for file_path, manual_counts, pdf_name in files_man:\n",
    "        # Read PDF content\n",
    "        pdf_df = spark.read.format(\"binaryFile\").load(file_path).cache()\n",
    "        binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"]\n",
    "        \n",
    "        # Extract text\n",
    "        extracted_text = extract_text(binary_pdf)\n",
    "        \n",
    "        # Clean and chunk text\n",
    "        cleaned_text = clean_text(extracted_text)\n",
    "        chunked_text = get_text_chunks(cleaned_text)\n",
    "        \n",
    "        # Run validation\n",
    "        _, accuracy = validate_tech_doc_recognition(nlp, cleaned_text, manual_counts)\n",
    "        pdf_accuracies[pdf_name] = accuracy\n",
    "        \n",
    "        print(f\"{pdf_name} Accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        # Clear cache\n",
    "        pdf_df.unpersist()\n",
    "    \n",
    "    return pdf_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc393e5a-6254-452f-9118-995e66940c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from typing import Dict, List, Tuple\n",
    "from builtins import min  # Explicitly import the built-in min function\n",
    "\n",
    "\n",
    "def normalize_tech_doc_id(doc_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes technical document IDs by removing spaces and converting to lowercase.\n",
    "    For example: 'MCE 0107 B' -> 'mce0107b'\n",
    "    \"\"\"\n",
    "    return ''.join(doc_id.split()).lower()\n",
    "\n",
    "def validate_tech_doc_recognition(nlp, text: str, manual_counts: Dict[str, int]) -> Tuple[PrettyTable, float]:\n",
    "    \"\"\"\n",
    "    Validates the NER model's performance on technical document recognition,\n",
    "    treating different format variations of the same document ID as equivalent.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # First, create normalized versions of manual counts and track variations\n",
    "    normalized_manual_counts = {}\n",
    "    variations_map = {}  # Maps normalized IDs to sets of original variations\n",
    "    \n",
    "    for original_id, count in manual_counts.items():\n",
    "        normalized_id = normalize_tech_doc_id(original_id)\n",
    "        \n",
    "        # Update normalized counts\n",
    "        if normalized_id not in normalized_manual_counts:\n",
    "            normalized_manual_counts[normalized_id] = 0\n",
    "            variations_map[normalized_id] = set()\n",
    "        \n",
    "        normalized_manual_counts[normalized_id] += count\n",
    "        variations_map[normalized_id].add(original_id)\n",
    "    \n",
    "    # Count model predictions, normalizing as we go\n",
    "    predicted_counts = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"TECH_DOC\":\n",
    "            normalized_ent = normalize_tech_doc_id(ent.text)\n",
    "            if normalized_ent not in predicted_counts:\n",
    "                predicted_counts[normalized_ent] = 0\n",
    "            predicted_counts[normalized_ent] += 1\n",
    "            \n",
    "            # Add this variation to our tracking if it's a new format\n",
    "            if normalized_ent in variations_map:\n",
    "                variations_map[normalized_ent].add(ent.text)\n",
    "    \n",
    "    # Create comparison table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\n",
    "        \"Technical Document\",\n",
    "        \"Variations Found\",\n",
    "        \"Manual Count\",\n",
    "        \"Model Count\",\n",
    "        \"Difference\",\n",
    "        \"Accuracy %\"\n",
    "    ]\n",
    "    table.align = \"l\"\n",
    "    \n",
    "    # Track totals\n",
    "    total_manual = 0\n",
    "    total_predicted = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Add rows for each unique normalized document ID\n",
    "    processed_ids = set()\n",
    "    \n",
    "    # First, process all manual counts\n",
    "    for normalized_id in normalized_manual_counts.keys():\n",
    "        if normalized_id in processed_ids:\n",
    "            continue\n",
    "            \n",
    "        processed_ids.add(normalized_id)\n",
    "        \n",
    "        manual_count = normalized_manual_counts[normalized_id]\n",
    "        predicted_count = predicted_counts.get(normalized_id, 0)\n",
    "        \n",
    "        # Get all variations found\n",
    "        variations = sorted(variations_map[normalized_id])\n",
    "        variations_str = \", \".join(variations)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = min(predicted_count, manual_count) / manual_count * 100 if manual_count > 0 else 0\n",
    "        \n",
    "        # Update totals\n",
    "        total_manual += manual_count\n",
    "        total_predicted += predicted_count\n",
    "        total_correct += min(predicted_count, manual_count)\n",
    "        \n",
    "        # Use the first variation as the primary ID for display\n",
    "        primary_id = sorted(variations_map[normalized_id])[0]\n",
    "        \n",
    "        table.add_row([\n",
    "            primary_id,\n",
    "            variations_str,\n",
    "            manual_count,\n",
    "            predicted_count,\n",
    "            predicted_count - manual_count,\n",
    "            f\"{accuracy:.1f}%\"\n",
    "        ])\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = (total_correct / total_manual * 100) if total_manual > 0 else 0\n",
    "    \n",
    "    # Add totals row\n",
    "    table.add_row([\n",
    "        \"TOTAL\",\n",
    "        \"\",\n",
    "        total_manual,\n",
    "        total_predicted,\n",
    "        total_predicted - total_manual,\n",
    "        f\"{overall_accuracy:.1f}%\"\n",
    "    ])\n",
    "    \n",
    "    return table, overall_accuracy\n",
    "\n",
    "# Run validation\n",
    "results_table, overall_accuracy = validate_tech_doc_recognition(nlp, big_chunk, manual_counts4)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80abfa3-6829-498b-a9c4-bffd74e2ca18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracies):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    pdfs = list(accuracies.keys())\n",
    "    acc_values = list(accuracies.values())\n",
    "    \n",
    "    bars = plt.bar(pdfs, acc_values, color='skyblue')\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Technical Document Recognition Accuracy Across PDFs', pad=20)\n",
    "    plt.xlabel('PDF Documents')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set y-axis range\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Add average line\n",
    "    avg_accuracy = np.mean(acc_values)\n",
    "    plt.axhline(y=avg_accuracy, color='r', linestyle='--', alpha=0.8)\n",
    "    plt.text(len(pdfs)-1, avg_accuracy, f'Average: {avg_accuracy:.1f}%', \n",
    "             va='bottom', ha='right', color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ffcdda3-c907-4580-9a43-1adac253f088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def main():\n",
    "    # Initialize spaCy pipeline\n",
    "    nlp = create_nlp_pipeline()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Run validation\n",
    "    results_table, overall_accuracy = validate_tech_doc_recognition(nlp, big_chunk, manual_counts4)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTechnical Document Recognition Validation\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_table)\n",
    "    print(f\"\\nOverall Model Accuracy: {overall_accuracy:.1f}%\")\n",
    "\n",
    "    # Get accuracies for all PDFs\n",
    "    accuracies = get_accuracies_for_all_pdfs(nlp)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_accuracies(accuracies)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "PDF LLMgit",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
