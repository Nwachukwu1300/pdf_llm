{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob\n",
    "\n",
    "%pip install pdfplumber\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "%pip install prettytable\n",
    "\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''USEFUL LINKS\n",
    "https://www.babelstreet.com/blog/what-is-entity-extraction#:~:text=Entity%20extraction%20(aka%2C%20named%20entity,%2C%20webpages%2C%20text%20fields).\n",
    "\n",
    "https://medium.com/@sanskrutikhedkar09/mastering-information-extraction-from-unstructured-text-a-deep-dive-into-named-entity-recognition-4aa2f664a453\n",
    "\n",
    "https://www.microfocus.com/documentation/relativity/relativity1217/reldbdsn/GUID-7C2DF185-41A1-4448-81E7-3252AA8DEBB3.html \n",
    "\n",
    "'''\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def create_nlp_pipeline():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    \n",
    "    patterns = [\n",
    "        {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\"]}},\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}$\"}},\n",
    "        {\"LOWER\": {\"IN\": [\":\"]}},  # Must have colon immediately after\n",
    "    ]},\n",
    "    # Pattern 1: Connected with optional letter (mce0107b or mce0107)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(mce|mch|mcx|mcg|tr|mcs|oa|og|os|pa|pt|pl|re|rg|se|trg|trh)\\d{4}[a-z]?$\"}}\n",
    "    ]},\n",
    "\n",
    "    # Pattern 2: Space after prefix (MCE 0107B or MCE 0107)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        # Match the prefix more flexibly\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\", \"mcs\", \"oa\", \"og\", \"os\", \"pa\", \"pt\", \"pl\", \"re\", \"rg\", \"se\", \"trg\", \"trh\"]}},\n",
    "        # Match any numbers with optional suffix, removing strict boundaries\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}[A-Za-z]?$\"}}\n",
    "    ]},\n",
    "\n",
    "    # Pattern 3: Prefix, number, and separate letter (mce 0107 b)\n",
    "    {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\", \"mcs\", \"oa\", \"og\", \"os\", \"pa\", \"pt\", \"pl\", \"re\", \"rg\", \"se\", \"trg\", \"trh\"]}},\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}$\"}},\n",
    "        {\"LOWER\": {\"REGEX\": r\"^[a-z]$\"}},\n",
    "        \n",
    "    ]},\n",
    "\n",
    "            {\"label\": \"TECH_DOC\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"mce\", \"mch\", \"mcx\", \"mcg\", \"tr\", \"mcs\", \"oa\", \"og\", \"os\", \"pa\", \"pt\", \"pl\", \"re\", \"rg\", \"se\", \"trg\", \"trh\"]}},\n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d{4}(?=\\\\)\"}}  # Matches only the 4 digits when followed by backslash\n",
    "    ]},\n",
    "\n",
    "    \n",
    "    # SYSTEM_COMPONENT Patterns\n",
    "    {\"label\": \"SYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(midas|nmcs2?|hadecs|hatms)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"midas\"}, \n",
    "        {\"LOWER\": \"gold\"}\n",
    "    ]},\n",
    "    \n",
    "    # HARDWARE_COMPONENT Patterns\n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(cabinet|plinth|lantern|post|frame|skirt)$\"}}, \n",
    "        {\"LOWER\": \"type\"}, \n",
    "        {\"TEXT\": {\"REGEX\": r\"^\\d+[a-z]?$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(ms[1-4]r?|ami|ert)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"HARDWARE_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"indicator\", \"signal\", \"sensor\", \"detector\", \"camera\", \"telephone\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # COMMUNICATION_COMPONENT Patterns\n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(rs485|rs422|tcp\\/ip|lan|wan)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"rs\"}, \n",
    "        {\"TEXT\": {\"REGEX\": r\"^(485|422)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"COMMUNICATION_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"ethernet\"}, \n",
    "        {\"LOWER\": {\"IN\": [\"lan\", \"connection\", \"interface\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # SUBSYSTEM_COMPONENT Patterns\n",
    "    {\"label\": \"SUBSYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"signal\", \"message\", \"meteorological\", \"tidal\", \"tunnel\"]}}, \n",
    "        {\"LOWER\": \"subsystem\"}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SUBSYSTEM_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"REGEX\": r\"^(lcc|pdu|cobs|ceclb|ceceb|cecr)$\"}}\n",
    "    ]},\n",
    "    \n",
    "    # CONTROL_COMPONENT Patterns\n",
    "    {\"label\": \"CONTROL_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"control\", \"monitoring\", \"outstation\", \"instation\"]}}, \n",
    "        {\"LOWER\": {\"IN\": [\"system\", \"unit\", \"equipment\", \"interface\"]}}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"CONTROL_COMPONENT\", \"pattern\": [\n",
    "        {\"LOWER\": \"cctv\"}, \n",
    "        {\"LOWER\": {\"IN\": [\"system\", \"camera\", \"equipment\"]}}\n",
    "    ]},\n",
    "    \n",
    "    # SPECIFICATION_TYPE Patterns\n",
    "    {\"label\": \"SPECIFICATION_TYPE\", \"pattern\": [\n",
    "        {\"LOWER\": {\"IN\": [\"requirements\", \"specification\", \"instructions\", \"overview\", \"process\"]}}, \n",
    "        {\"LOWER\": \"document\"}\n",
    "    ]},\n",
    "    \n",
    "    {\"label\": \"SPECIFICATION_TYPE\", \"pattern\": [\n",
    "        {\"LOWER\": \"technical\"}, \n",
    "        {\"LOWER\": \"requirements\"}\n",
    "    ]}\n",
    "]\n",
    "    \n",
    "    ruler.add_patterns(patterns)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_text(binary_data):\n",
    "  pdf_file = BytesIO(binary_data)\n",
    "\n",
    "  with pdfplumber.open(pdf_file) as pdf:\n",
    "    plain_text = \"\"\n",
    "\n",
    "    for page in pdf.pages:\n",
    "      plain_text += page.extract_text()\n",
    "\n",
    "    return plain_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"] \n",
    "\n",
    "extracted_text = extract_text(binary_pdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = create_nlp_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_aware_chunks(text: str, max_tokens: int = 300) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks while preserving document structure and page breaks.\n",
    "    Specifically handles page indicators that contain document references.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to split\n",
    "        max_tokens: Approximate maximum tokens per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks that respect document structure\n",
    "    \"\"\"\n",
    "    import tiktoken\n",
    "    import re\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Define pattern for section headers and page indicators\n",
    "    header_patterns = [\n",
    "        r'^#+\\s+.+$',                      # Markdown headers\n",
    "        r'^[A-Z0-9][.)\\s]+[A-Z].*$',       # Numbered sections like \"1. INTRODUCTION\"\n",
    "        r'^[IVXLCDMivxlcdm]+\\.\\s+.+$',     # Roman numeral sections\n",
    "        r'^Section\\s+\\d+[.:]\\s+.+$',       # Explicit section markers\n",
    "        r'^\\d+\\.\\d+\\s+.+$'                 # Decimal numbering like \"1.2 Configuration\"\n",
    "    ]\n",
    "    \n",
    "    # Pattern for page indicators that might contain document references\n",
    "    page_patterns = [\n",
    "        r'.*page\\s+\\d+.*',                 # \"page X\" indicators\n",
    "        r'.*\\b(mce|mch|mcx|mcg|tr)\\s*\\d{4}[a-z]?\\b.*\\bpage\\b', # Doc ref + page\n",
    "        r'.*\\d+\\s*of\\s*\\d+\\s*$'            # \"X of Y\" page counters\n",
    "    ]\n",
    "    \n",
    "    # Identify structural elements\n",
    "    lines = text.split('\\n')\n",
    "    structure = []\n",
    "    current_section = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if not line.strip():  # Empty line\n",
    "            if current_section:\n",
    "                current_section.append(line)\n",
    "            continue\n",
    "        \n",
    "        # Check if this is a page indicator\n",
    "        is_page_indicator = any(re.match(pattern, line.lower(), re.MULTILINE) for pattern in page_patterns)\n",
    "        \n",
    "        # If it's a page indicator, add a space before and after\n",
    "        if is_page_indicator:\n",
    "            # If not the first line, add space before\n",
    "            if i > 0 and current_section:\n",
    "                if not current_section[-1].endswith(' ') and not current_section[-1].endswith('\\n'):\n",
    "                    current_section[-1] += ' '\n",
    "            \n",
    "            # Add the page indicator with space after\n",
    "            current_section.append(line + ' ')\n",
    "            current_tokens += len(encoding.encode(line + ' '))\n",
    "            \n",
    "            # Force a section break after page indicators\n",
    "            if current_section:\n",
    "                structure.append('\\n'.join(current_section))\n",
    "                current_section = []\n",
    "                current_tokens = 0\n",
    "            continue\n",
    "            \n",
    "        line_tokens = len(encoding.encode(line))\n",
    "        \n",
    "        # Check if this is a header\n",
    "        is_header = any(re.match(pattern, line.strip(), re.MULTILINE) for pattern in header_patterns)\n",
    "        \n",
    "        # If header or we'd exceed token limit, start new section\n",
    "        if is_header or (current_tokens + line_tokens > max_tokens and current_section):\n",
    "            if current_section:\n",
    "                structure.append('\\n'.join(current_section))\n",
    "                current_section = []\n",
    "                current_tokens = 0\n",
    "        \n",
    "        # Add line to current section\n",
    "        current_section.append(line)\n",
    "        current_tokens += line_tokens\n",
    "        \n",
    "        # If we're at token limit, break section\n",
    "        if current_tokens >= max_tokens:\n",
    "            structure.append('\\n'.join(current_section))\n",
    "            current_section = []\n",
    "            current_tokens = 0\n",
    "    \n",
    "    # Add any remaining content\n",
    "    if current_section:\n",
    "        structure.append('\\n'.join(current_section))\n",
    "    \n",
    "    # Merge small chunks but respect page breaks\n",
    "    merged_structure = []\n",
    "    current_chunk = \"\"\n",
    "    current_chunk_tokens = 0\n",
    "    \n",
    "    for section in structure:\n",
    "        # Check if this section contains a page indicator\n",
    "        contains_page_indicator = any(re.search(pattern, section.lower()) for pattern in page_patterns)\n",
    "        \n",
    "        section_tokens = len(encoding.encode(section))\n",
    "        \n",
    "        # Don't merge if this contains a page indicator or previous chunk ends with one\n",
    "        if contains_page_indicator or any(re.search(pattern, current_chunk.lower()) for pattern in page_patterns):\n",
    "            # Add previous chunk if it exists\n",
    "            if current_chunk:\n",
    "                merged_structure.append(current_chunk)\n",
    "            \n",
    "            # Start new chunk with this section\n",
    "            current_chunk = section\n",
    "            current_chunk_tokens = section_tokens\n",
    "        # Otherwise check if it can be merged\n",
    "        elif section_tokens < max_tokens * 0.25 and current_chunk_tokens + section_tokens <= max_tokens:\n",
    "            current_chunk += \"\\n\\n\" + section if current_chunk else section\n",
    "            current_chunk_tokens += section_tokens\n",
    "        else:\n",
    "            # Add previous chunk if it exists\n",
    "            if current_chunk:\n",
    "                merged_structure.append(current_chunk)\n",
    "            \n",
    "            # Start new chunk with this section\n",
    "            current_chunk = section\n",
    "            current_chunk_tokens = section_tokens\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        merged_structure.append(current_chunk)\n",
    "    \n",
    "    return merged_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_cluster_chunks(text: str, max_tokens: int = 300, similarity_threshold: float = 0.7) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks based on semantic similarity, keeping related content together.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to split (must be a string, not a list)\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "        similarity_threshold: Threshold for considering paragraphs semantically related\n",
    "        \n",
    "    Returns:\n",
    "        List of semantically coherent text chunks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tiktoken\n",
    "        import numpy as np\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        import re\n",
    "        \n",
    "        # Ensure input is a string\n",
    "        if isinstance(text, list):\n",
    "            text = \" \".join(text)\n",
    "            \n",
    "        # Initialize tokenizer\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Split text into paragraphs\n",
    "        paragraphs = [p for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n",
    "        \n",
    "        # Handle empty input\n",
    "        if not paragraphs:\n",
    "            return []\n",
    "            \n",
    "        # Calculate token counts for each paragraph\n",
    "        paragraph_tokens = [len(encoding.encode(p)) for p in paragraphs]\n",
    "        \n",
    "        # Compute semantic similarity between paragraphs\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        \n",
    "        # Handle case when there's only one paragraph\n",
    "        if len(paragraphs) == 1:\n",
    "            return paragraphs\n",
    "            \n",
    "        tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Cluster paragraphs by semantic similarity\n",
    "        clusters = []\n",
    "        visited = set()\n",
    "        \n",
    "        for i in range(len(paragraphs)):\n",
    "            if i in visited:\n",
    "                continue\n",
    "                \n",
    "            # Start a new cluster\n",
    "            cluster = [i]\n",
    "            visited.add(i)\n",
    "            \n",
    "            # Find semantically similar paragraphs\n",
    "            for j in range(len(paragraphs)):\n",
    "                if j in visited:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if semantically similar to any paragraph in current cluster\n",
    "                if any(similarity_matrix[k, j] >= similarity_threshold for k in cluster):\n",
    "                    cluster.append(j)\n",
    "                    visited.add(j)\n",
    "            \n",
    "            clusters.append(sorted(cluster))\n",
    "        \n",
    "        # Create chunks based on clusters while respecting token limits\n",
    "        chunks = []\n",
    "        for cluster in clusters:\n",
    "            cluster_paragraphs = [paragraphs[i] for i in cluster]\n",
    "            cluster_tokens = [paragraph_tokens[i] for i in cluster]\n",
    "            \n",
    "            # If cluster is too big, split it by semantic distance\n",
    "            if sum(cluster_tokens) > max_tokens:\n",
    "                # Sort paragraphs by semantic distance to the first paragraph\n",
    "                first_idx = cluster[0]\n",
    "                sorted_by_similarity = sorted(\n",
    "                    cluster,\n",
    "                    key=lambda idx: -similarity_matrix[first_idx, idx]\n",
    "                )\n",
    "                \n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "                \n",
    "                for idx in sorted_by_similarity:\n",
    "                    para = paragraphs[idx]\n",
    "                    tokens = paragraph_tokens[idx]\n",
    "                    \n",
    "                    if current_tokens + tokens > max_tokens and current_chunk:\n",
    "                        chunks.append(\"\\n\\n\".join(current_chunk))\n",
    "                        current_chunk = []\n",
    "                        current_tokens = 0\n",
    "                        \n",
    "                    current_chunk.append(para)\n",
    "                    current_tokens += tokens\n",
    "                    \n",
    "                if current_chunk:\n",
    "                    chunks.append(\"\\n\\n\".join(current_chunk))\n",
    "            else:\n",
    "                # Add whole cluster as one chunk\n",
    "                chunks.append(\"\\n\\n\".join(cluster_paragraphs))\n",
    "        \n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        # Return original text as a single chunk on error\n",
    "        print(f\"Semantic chunking failed with error: {e}\")\n",
    "        if isinstance(text, list):\n",
    "            return text\n",
    "        else:\n",
    "            return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_character_chunks(text: str, max_chunk_size: int = 300) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively split text into chunks based on characters without overlap.\n",
    "    Tries to split at paragraph/sentence boundaries when possible.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to split\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Base case: if text fits in a single chunk\n",
    "    if len(text) <= max_chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    # Try to find a paragraph break\n",
    "    last_para = text[:max_chunk_size].rfind(\"\\n\\n\")\n",
    "    if last_para != -1 and last_para > max_chunk_size // 2:\n",
    "        # Split at paragraph\n",
    "        first_chunk = text[:last_para]\n",
    "        rest = text[last_para:].lstrip()\n",
    "    else:\n",
    "        # Try to find a sentence break\n",
    "        for sep in [\". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\", \"\\n\"]:\n",
    "            last_sent = text[:max_chunk_size].rfind(sep)\n",
    "            if last_sent != -1 and last_sent > max_chunk_size // 2:\n",
    "                first_chunk = text[:last_sent + 1]  # Include the separator\n",
    "                rest = text[last_sent + 1:].lstrip()\n",
    "                break\n",
    "        else:\n",
    "            # Last resort: split at word boundary\n",
    "            last_space = text[:max_chunk_size].rfind(\" \")\n",
    "            if last_space != -1 and last_space > max_chunk_size // 3:\n",
    "                first_chunk = text[:last_space]\n",
    "                rest = text[last_space:].lstrip()\n",
    "            else:\n",
    "                # No good break point found, just split at max size\n",
    "                first_chunk = text[:max_chunk_size]\n",
    "                rest = text[max_chunk_size:]\n",
    "    \n",
    "    # Recursively process the rest of the text\n",
    "    return [first_chunk] + recursive_character_chunks(rest, max_chunk_size)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def recursive_token_chunks(text: str, max_tokens: int = 200) -> List[str]:\n",
    "    import tiktoken\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Get tokens for the full text\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    # Base case: if text fits in a single chunk\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    # Get the text for the maximum token size\n",
    "    potential_chunk_text = encoding.decode(tokens[:max_tokens])\n",
    "    \n",
    "    # Try to find a paragraph break\n",
    "    last_para = potential_chunk_text.rfind(\"\\n\\n\")\n",
    "    if last_para != -1 and last_para > len(potential_chunk_text) // 2:\n",
    "        # Split at paragraph\n",
    "        first_chunk = potential_chunk_text[:last_para]\n",
    "        # Count tokens in the first chunk\n",
    "        first_chunk_tokens = len(encoding.encode(first_chunk))\n",
    "        # Get the rest of the text\n",
    "        rest = encoding.decode(tokens[first_chunk_tokens:])\n",
    "    else:\n",
    "        # Try to find a sentence break\n",
    "        for sep in [\". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\", \"\\n\"]:\n",
    "            last_sent = potential_chunk_text.rfind(sep)\n",
    "            if last_sent != -1 and last_sent > len(potential_chunk_text) // 2:\n",
    "                first_chunk = potential_chunk_text[:last_sent + len(sep)]\n",
    "                first_chunk_tokens = len(encoding.encode(first_chunk))\n",
    "                rest = encoding.decode(tokens[first_chunk_tokens:])\n",
    "                break\n",
    "        else:\n",
    "            # Last resort: split at word boundary\n",
    "            last_space = potential_chunk_text.rfind(\" \")\n",
    "            if last_space != -1 and last_space > len(potential_chunk_text) // 3:\n",
    "                first_chunk = potential_chunk_text[:last_space]\n",
    "                first_chunk_tokens = len(encoding.encode(first_chunk))\n",
    "                rest = encoding.decode(tokens[first_chunk_tokens:])\n",
    "            else:\n",
    "                # No good break point found, just split at max tokens\n",
    "                first_chunk = potential_chunk_text\n",
    "                rest = encoding.decode(tokens[max_tokens:])\n",
    "    \n",
    "    # Recursively process the rest of the text\n",
    "    return [first_chunk] + recursive_token_chunks(rest, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def token_chunks_cl100k(text: str, tokens_per_chunk: int = 200) -> List[str]:        \n",
    "  encoding = tiktoken.get_encoding(\"cl100k_base\")       \n",
    "  tokens = encoding.encode(text)\n",
    "  chunks = []\n",
    "  for i in range(0, len(tokens), tokens_per_chunk):\n",
    "    chunk_tokens = tokens[i:i + tokens_per_chunk]\n",
    "    chunk_text = encoding.decode(chunk_tokens)\n",
    "    chunks.append(chunk_text)\n",
    "        \n",
    "  return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "  text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0, \n",
    "    separator=\"\\n\", \n",
    "    length_function=len\n",
    "    )\n",
    "  chunks = text_splitter.split_text(text)\n",
    "  return chunks\n",
    "\n",
    "\n",
    "def get_text_chunks2(text):\n",
    "    chunks = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "  if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "  cleaned_text = \" \".join(text.split())\n",
    "  cleaned_text = cleaned_text.lower()\n",
    "  return cleaned_text\n",
    "\n",
    "def lower_text(text):\n",
    "  lowertext = text.lower()\n",
    "  return lowertext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_text(extracted_text: str) -> List[str]:\n",
    "    chunks = get_text_chunks(extracted_text)\n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "        \n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "main_text = process_pdf_text(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of chunks:\", len(main_text))\n",
    "for i, chunk in enumerate(main_text, 1):\n",
    "    print(f\"\\nCHUNK {i}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the cleaning function\n",
    "test_text = \"\"\"\n",
    "The TR 2144       M:3952 document specifies requirements.\n",
    "Also see MCE 1234:123 and TR 2144 M for details!.\n",
    "Some MCE1234B document and TR 2144M reference.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(clean_text(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def debug_document_code(nlp, text):\n",
    "    \"\"\"\n",
    "    Provides detailed analysis of how document codes are being processed.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing document code: '{text}'\")\n",
    "    \n",
    "    # shows raw tokenization\n",
    "    doc = nlp(text)\n",
    "    print(\"\\nTokenization details:\")\n",
    "    for token in doc:\n",
    "        print(f\"Token: '{token.text}'\")\n",
    "        print(f\"  Position: {token.idx} to {token.idx + len(token.text)}\")\n",
    "        print(f\"  Is part of entity: {token.ent_type_ != ''}\")\n",
    "        print(f\"  Entity type: {token.ent_type_ if token.ent_type_ else 'None'}\")\n",
    "        print()\n",
    "    \n",
    "    # complete entities found\n",
    "    print(\"\\nComplete entities found:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Entity: '{ent.text}'\")\n",
    "        print(f\"  Label: {ent.label_}\")\n",
    "        print(f\"  Includes all tokens: {all(t.ent_type_ == ent.label_ for t in ent)}\")\n",
    "        print()\n",
    "    \n",
    "    # what didn't match\n",
    "    unmatched = [t.text for t in doc if not t.ent_type_]\n",
    "    if unmatched:\n",
    "        print(\"\\nUnmatched tokens:\")\n",
    "        print(\", \".join(unmatched))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_chunk = \"\"\"\n",
    "re 1110 The MCE0107B document connects to RS485 while MCH 1070B uses RS 422.\n",
    "MIDAS Gold system interfaces with the Ethernet LAN through Cabinet Type 600.\n",
    "The Signal Subsystem monitors the CCTV System and AMI-EE devices. (AMI bobo)\n",
    "\"\"\"\n",
    "\n",
    "medium_chunk = \"\"\"\n",
    "The assembly manual for MCH0107B specifies that RS485 components must be configured \n",
    "alongside RS422 adapters, with additional references to OA 0150C outlined in \n",
    "Section 4.3 of the document. TR 2043 further details the integration with \n",
    "Ethernet LAN systems, enabling high-speed communication protocols compliant \n",
    "with IEEE 802.3 standards. The MIDAS Gold system connects through Cabinet Type 600A \n",
    "to the Control System, while monitoring occurs via the CCTV System and Signal Subsystem.\n",
    "\n",
    "The MCE 1070B requirements document describes interfacing with NMCS2 through standard \n",
    "protocols. Local connections use RS 485 for primary communication, supported by \n",
    "AMI-EE devices and monitored by the Outstation Equipment.\n",
    "\"\"\"\n",
    "\n",
    "large_chunk = \"\"\"\n",
    "Technical Requirements Document: System Integration Specification\n",
    "\n",
    "1. Overview\n",
    "The MCE0107B specification, in conjunction with MCH 1070B and TR 2043, defines the \n",
    "integration requirements for the MIDAS Gold system. Primary communication occurs through \n",
    "RS485 interfaces, while secondary protocols utilize RS 422 and Ethernet LAN connections.\n",
    "\n",
    "2. Hardware Components\n",
    "Cabinet Type 600 houses the main control units, with additional Cabinet Type 450A units \n",
    "for auxiliary systems. The AMI-EE devices interface with MS3R indicators and standard \n",
    "AMI units. Signal sensors and detector units provide environmental monitoring capabilities.\n",
    "\n",
    "3. System Architecture\n",
    "The NMCS2 framework integrates with HADECS and HATMS subsystems through standardized \n",
    "interfaces. The Signal Subsystem and Message Subsystem handle primary control operations, \n",
    "while the Meteorological Subsystem provides environmental data. CECLB and CECEB units \n",
    "coordinate with the PDU for power distribution.\n",
    "\n",
    "4. Communication Infrastructure\n",
    "Primary TCP/IP networks connect through LAN and WAN interfaces. The Ethernet LAN provides \n",
    "local connectivity, supported by RS485 and RS 422 serial connections. Each Control System \n",
    "interfaces with its respective Monitoring Unit through dedicated channels.\n",
    "\n",
    "5. Monitoring and Control\n",
    "The CCTV System provides visual monitoring capabilities, integrated with the Control System \n",
    "and Monitoring Equipment. Outstation Equipment handles remote operations, while the \n",
    "Instation Interface manages central control functions.\n",
    "\n",
    "6. Reference Documentation\n",
    "MCE 1080B describes the detailed protocols, while TR 2044 and MCH 1075B provide \n",
    "supplementary specifications. The Requirements Document and Technical Requirements \n",
    "specify additional integration parameters.\n",
    "\n",
    "7. System Components\n",
    "Multiple AMI-EE installations connect through Cabinet Type 600B units, monitored by \n",
    "the Signal Subsystem. The MIDAS Gold deployment utilizes standard NMCS2 protocols for \n",
    "primary operations.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_labels = ['TECH_DOC', 'SYSTEM_COMPONENT', 'HARDWARE_COMPONENT', 'COMMUNICATION_COMPONENT', 'SUBSYSTEM_COMPONENT', 'CONTROL_COMPONENT', 'SPECIFICATION_TYPE']\n",
    "\n",
    "\n",
    "def process_text(nlp, text: str):\n",
    "    \"\"\"\n",
    "    Process text and return detailed entity information including:\n",
    "    - Individual entity frequencies\n",
    "    - Entity type counts\n",
    "    - Context and position information\n",
    "    \n",
    "    This enhanced tracking helps build a more informed knowledge graph by showing\n",
    "    which specific entities are most referenced in the documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # tracking dictionaries\n",
    "    entity_type_counts = {label: 0 for label in custom_labels}  # Counts by entity type\n",
    "    entity_frequencies = {}  # Counts of specific entity mentions\n",
    "    \n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in custom_labels:\n",
    "            # Create entity record\n",
    "            entity_info = {\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'original_text': text[ent.start_char:ent.end_char],\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char\n",
    "            }\n",
    "            entities.append(entity_info)\n",
    "            \n",
    "            entity_type_counts[ent.label_] += 1\n",
    "            \n",
    "            # Update specific entity frequency\n",
    "            entity_key = (ent.text, ent.label_)  # Tuple of text and label to handle same text with different labels\n",
    "            if entity_key not in entity_frequencies:\n",
    "                entity_frequencies[entity_key] = {\n",
    "                    'count': 0,\n",
    "                    'text': ent.text,\n",
    "                    'label': ent.label_\n",
    "                }\n",
    "            entity_frequencies[entity_key]['count'] += 1\n",
    "    \n",
    "    return entities, entity_type_counts, entity_frequencies\n",
    "\n",
    "def print_document_results(entities, type_counts, frequencies):\n",
    "    \"\"\"\n",
    "    Display comprehensive entity analysis including:\n",
    "    - Individual entities found\n",
    "    - Counts by entity type\n",
    "    - Frequency of specific entities\n",
    "    \"\"\"\n",
    "    print(\"\\nDocument Processing Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"Entities Found in Context:\")\n",
    "    for entity in entities:\n",
    "        print(f\"Found: {entity['text']} ({entity['label']})\")\n",
    "        print(f\"Original text: '{entity['original_text']}'\")\n",
    "        print(f\"Position: {entity['start']} to {entity['end']}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\\nEntity Type Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    for label, count in type_counts.items():\n",
    "        if count > 0:  \n",
    "            print(f\"{label}: {count} total mentions\")\n",
    "    \n",
    "    print(\"\\nDetailed Entity Frequencies:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Group frequencies by entity type for clearer presentation\n",
    "    grouped_frequencies = {}\n",
    "    for (text, label), info in frequencies.items():\n",
    "        if label not in grouped_frequencies:\n",
    "            grouped_frequencies[label] = []\n",
    "        grouped_frequencies[label].append(info)\n",
    "    \n",
    "    # Print frequencies by type\n",
    "    for label in custom_labels:\n",
    "        if label in grouped_frequencies:\n",
    "            print(f\"\\n{label}:\")\n",
    "            # Sort by frequency, highest first\n",
    "            sorted_entities = sorted(grouped_frequencies[label], \n",
    "                                  key=lambda x: x['count'], \n",
    "                                  reverse=True)\n",
    "            for entity in sorted_entities:\n",
    "                print(f\"  {entity['text']}: {entity['count']} mentions\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the NLP pipeline\n",
    "#nlp = create_nlp_pipeline()\n",
    "\n",
    "debug_document_code(nlp, \"MCE0107B , MCE2344B x  TR 0543 C more, MCE 2343: bro nn, MCE 3342C cap, MCE 3333 B bag, MCE3234 A stir tr 2144 m:3952 test this he Lane SAC Priority Data section contains the priority table for lane SAC settings (see TR 2163\\I:410). \") \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_chunk = str(small_chunk)\n",
    " \n",
    "entities, counts, freq = process_text(nlp, s_chunk)\n",
    "print_document_results(entities, counts, freq)\n",
    "\n",
    "debug_document_code(nlp, s_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "big_chunk = str(main_text)\n",
    " \n",
    "entities, counts, freq = process_text(nlp, big_chunk)\n",
    "print_document_results(entities, counts, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(big_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manual_counts1 = {'TR 1100': 10, 'TR 2070': 8, 'TR 2142': 4, 'TR 2043': 3, 'TR 2067': 4, 'TR 2130': 3, 'TR2070': 3, 'TR 2072': 2, 'TR2139': 1, 'MCE 1349': 3, 'MCE0110': 1, 'MCE0107': 3, 'MCH 1618': 2, 'MCX 0731': 1, 'MCX 0925': 1, 'MCX 0910': 1, 'TRH 1679':1, 'TRH 1680':4, 'TRG 0500':1}\n",
    "\n",
    "\n",
    "manual_counts2 = {\n",
    "        'TR 2033': 5,'TR 2043': 14,'TR 1100': 12,'TR 2070': 6,'TR 2130': 5,'TR 2142': 5,'TR 2067': 3,'TR2070': 4,'TR 2072': 2,'TR2139': 1,'TR 1173': 1,'TR 1238': 1,'TR 2110': 1,'MCX1031':17,'MCX0920': 3,'MCX0918': 1,'MCX0733': 1,'MCH 1618': 2,'MCH 1689': 2,'MCH1349': 1,'MCH 1621': 1,'MCE 0110': 1,'MCE0110': 2,'MCE0107': 1,'MCG 1069': 1,'TRH 1679':2, 'TRG 0500':2}\n",
    "\n",
    "manual_counts3 = {'mce 1157': 2, 'mce 1157 a': 1, 'mce 1157 b': 1, 'mce 1157 c': 1, 'mce 1157 d': 1, 'mce 1157 e': 1, 'tr 1100': 7, 'mcx 0708': 4, 'tr 2130': 4, 'mch 1616': 3, 'mch 1618': 3, 'tr 2199': 3, 'mcg 1107': 2, 'mch 1349': 2, 'tr2199': 1}\n",
    "\n",
    "manual_counts4 = {'mch 1744': 23, 'tr 2144': 11, 'mcg 1091': 3, 'tr2144': 3, 'mcg 1069': 3, 'mcg 1092': 2, 'mcg 1093': 1, 'mch 1714': 1, 'mch 1753': 1, 'mch 1748': 1}\n",
    "\n",
    "manual_counts5 = {'tr 2199': 138, 'tr 2130': 16, 'tr 1100': 14, 'tr 2067': 13, 'tr 2070': 10, 'mcg 1069': 5, 'tr 2516': 5, 'mce 1137': 4, 'mch 1689': 3, 'mcx 0028': 3, 'tr 2195': 2, 'mcx 0071': 2, 'tr 2045': 2, 'mcg 1107': 1, 'mch 1616': 1}\n",
    "\n",
    "manual_counts6 = {'mcg 1069': 3, 'mce 0110': 3, 'tr 1100': 2, 'tr 2199': 2, 'mce 0107': 2, 'tr 2195': 2, 'mce 2214': 2, 'mch 1616': 2, 'mcg 1202': 1}\n",
    "\n",
    "manual_counts7 = {'mch 1753': 13, 'tr 2144': 4, 'mch 1744': 1}\n",
    "\n",
    "manual_counts8 = {'mch 1748': 39, 'tr 2163': 23, 'tr 2133': 5, 'mch 1726': 4, 'tr 2139': 3, 'mch 1689': 3, 'mch 1617': 2, 'mch 1618': 2, 'mch 1655': 2, 'tr 2072': 1, 'mch 1700': 2, 'mch 1759': 1, 'mch1619': 1, 'mch 1124': 1}\n",
    "\n",
    "manual_counts9 = {'mch 1748': 38, 'mch 1689': 14, 'mce 2103': 4, 'mch 1700': 4, 'mch 1616': 3, 'tr 2072': 3, 'tr 2133': 2, 'mch 1798': 1, 'mch 1619': 1}\n",
    "\n",
    "manual_counts10 = {'mch2624': 4, 'mch1689': 2, 'mch 2629': 1}\n",
    "\n",
    "manual_counts11 = {\"mce 2240\": 14, \"mcg 1110\": 8, \"mce 2536\": 5, \"tr 1100\": 7, \"tr 2130\": 4, \"mce 1126\": 4, \"mce 2135\": 3, \"tr 2180\": 3, \"mce 1233\": 3, \"mce 0110\": 2, \"mch 1514\": 2, \"tr 2189\": 3, \"mch 1619\": 1}\n",
    "\n",
    "manual_counts12 = {\"mce 2242\": 8, \"mce 2245\": 6, \"mce 2240\": 4, \"mce2247\": 4, \"mce 2247\": 3, \"mce 2239\": 1, \"mce 2241\": 1, \"mce 2135\": 1, \"mce 2013\": 1, \"mce 2216\": 1, \"mce 1959\": 1, \"mch 1959\": 4, \"mch 1960\": 1, \"mch 1970\": 1, \"mcg 1110\": 1}\n",
    "\n",
    "manual_counts13 = {\"tr 2145\": 18, \"tr 2172\": 9, \"tr 2173\": 8, \"tr 1100 c\": 1, \"tr 2145 d\": 1, \"tr 2172 d\": 1, \"tr 2173 j\": 1, \"re 2177 g\": 1}\n",
    "\n",
    "manual_counts14 = {\"mch 1700\": 18, \"mch 1596\": 2, \"mcg 1069\": 7, \"mce 2103\": 5, \"mcg 1075\": 3, \"mcg 1086\": 3, \"mcg 1091\": 3, \"mcg 1077\": 2}\n",
    "\n",
    "manual_counts18 = {\"mch 1781\": 2, \"mce2242\": 3, \"mcg 1110\": 1, \"mch 1960\": 1, \"mch 1970\": 1, \"mch 1959\": 1, \"mch 1731\": 1, \"mce 2239\": 1, \"mce 2240\": 1, \"mce 2241\": 1, \"mce 2216\": 1, \"mce 2247\": 1, \"mce 2242\": 1, \"mce 2246\": 1}\n",
    "\n",
    "\n",
    "manual_counts19={'mch 1760':9, 'mch1865':6, 'mch 1952':3, 'mch 1951':3, 'mch 1957':3, 'mch 1696':2, 'mch 1867':2, 'mch 1857':1,}\n",
    "\n",
    "manual_counts20 = {\"mch 2470\": 6, \"mch1965\": 5, \"mch 2474\": 4, \"mch 2472\": 3, \"mch 2473\": 3, \"mch1349\": 3, \"mch2471\": 2, \"mch1514\": 2, \"mch1144\": 2, \"mch1147\": 2, \"mch1148\": 2, \"mch 1965\": 1, \"mch2472\": 1, \"mch 2471\": 1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copilot_count1 = {'MCE0110': 1,'MCH1618': 2,'MCE0107': 3,'TR2043': 3,'TR2067': 4,'TR2070': 11,'TR2072': 2,'TR1100': 9,'MCE1349': 3,'TR2130': 3,'MCX0731': 1,'MCX0925': 1,'TR2033': 2,'MCX0910': 1,'TR2142': 4,'MCG1069': 1,'TR2139': 1\n",
    "}\n",
    "\n",
    "copilot_count2 = {'MCE0107': 1, 'MCE0110': 3, 'MCH1618': 2, 'MCX1031': 15, 'TR2043': 13, 'MCH1689': 2, 'TR2070': 9, 'MCX0920': 3, 'TR1100': 12, 'MCH1349': 1, 'TR2130': 4, 'TR2067': 3, 'MCX0918': 1, 'TR2033': 4, 'MCX0733': 1, 'TR2142': 5, 'MCG1069': 1, 'TR2072': 2, 'TR2139': 1, 'MCH1621': 1, 'TR1173': 1, 'TR1238': 1, 'TR2110': 1, 'MCE 0107 B': 1, 'MCX 1031': 2}\n",
    "\n",
    "copilot_count3 = {'MCE1157': 7, 'MCH1616': 3, 'MCH1618': 3, 'TR2199': 4, 'MCG1107': 2, 'TR1100': 7, 'MCX0708': 4, 'MCH1349': 2, 'TR2130': 4}\n",
    "\n",
    "copilot_count4 = {'MCG1093': 1, 'TR2144': 14, 'MCH1744': 23, 'MCG1069': 3, 'MCG1091': 3, 'MCG1092': 2, 'MCH1714': 1, 'MCH1753': 1, 'MCH1748': 1}\n",
    "\n",
    "copilot_count6 = {'MCG1202': 1, 'TR1100': 2, 'MCG1069': 3, 'TR2199': 2, 'MCE0107': 2, 'TR2195': 2, 'MCE2214': 2, 'MCE0110': 3, 'MCH1616': 2}\n",
    "\n",
    "copilot_count7 = {'MCH1744': 1, 'TR2144': 4, 'MCH1753': 11}\n",
    "\n",
    "copilot_count8= {'MCH1759': 1, 'MCH1726': 4, 'MCH1617': 2, 'MCH1618': 2, 'MCH1655': 2, 'TR2139': 3, 'TR2163': 23, 'MCH1748': 39, 'MCH1689': 3, 'TR2133': 5, 'MCH1700': 2, 'TR2072': 1, 'MCH1619': 1, 'MCH1124': 1}\n",
    "\n",
    "copilot_count9 = {'MCH1798': 1, 'MCH1616': 3, 'MCH1748': 38, 'MCE2103': 4, 'MCH1689': 14, 'TR2072': 3, 'MCH1700': 4, 'TR2133': 1, 'MCH1619': 1}\n",
    "\n",
    "copilot_count10 = {'MCH2629': 1, 'MCH2624': 4, 'MCH1689': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies_for_all_pdfs(nlp):\n",
    "    pdf_accuracies = {}\n",
    "    \n",
    "    # List of all files and their corresponding manual counts\n",
    "    files_man = [\n",
    "        (file1, manual_counts1, \"MCE0110B\"),\n",
    "        (file2, manual_counts2, \"MCE0107B\"),\n",
    "        (file3, manual_counts3, \"MCE1157E\"),\n",
    "        (file4, manual_counts4, \"MCG1093J\"),\n",
    "        (file5, manual_counts5, \"MCG1107B\"),\n",
    "        (file6, manual_counts6, \"MCG1202A\"),\n",
    "        (file7, manual_counts7, \"MCH1744H\"),\n",
    "        (file8, manual_counts8, \"MCH1759F\"),\n",
    "        (file9, manual_counts9, \"MCH1798H\"),\n",
    "        (file10, manual_counts10, \"MCH2629A\")\n",
    "    ]\n",
    "\n",
    "    files_man2 = [\n",
    "    (file11, manual_counts11, \"MCE2241F\"),\n",
    "    (file12, manual_counts12, \"MCE2246A\"),\n",
    "    (file13, manual_counts13, \"MCG1090D\"),\n",
    "    (file14, manual_counts14, \"MCG1094C\"),\n",
    "    (file18, manual_counts18, \"MCH1734A\"),\n",
    "    (file19, manual_counts19, \"MCH1948B\"),\n",
    "    (file20, manual_counts20, \"MCH2475C\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    files_copilot = [\n",
    "        (file1, copilot_count1, \"MCE0110B\"),\n",
    "        (file2, copilot_count2, \"MCE0107B\"),\n",
    "        (file3, copilot_count3, \"MCE1157E\"),\n",
    "        (file4, copilot_count4, \"MCG1093J\"),\n",
    "        (file6, copilot_count6, \"MCG1202A\"),\n",
    "        (file7, copilot_count7, \"MCH1744H\"),\n",
    "        (file8, copilot_count8, \"MCH1759F\"),\n",
    "        (file9, copilot_count9, \"MCH1798H\"),\n",
    "        (file10, copilot_count10, \"MCH2629A\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    \n",
    "    for file_path, manual_counts, pdf_name in files_man:\n",
    "        # Read PDF content\n",
    "        pdf_df = spark.read.format(\"binaryFile\").load(file_path).cache()\n",
    "        binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"]\n",
    "        \n",
    "        # Extract text\n",
    "        extracted_text = extract_text(binary_pdf)\n",
    "        \n",
    "        text_for_validation = process_pdf_text(extracted_text)\n",
    "\n",
    "        # Run validation\n",
    "        _, accuracy = validate_tech_doc_recognition(nlp, text_for_validation, manual_counts)\n",
    "        pdf_accuracies[pdf_name] = accuracy\n",
    "        \n",
    "        print(f\"{pdf_name} Accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        # Clear cache\n",
    "        pdf_df.unpersist()\n",
    "    \n",
    "    return pdf_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from typing import Dict, List, Tuple\n",
    "from builtins import min, abs\n",
    "\n",
    "\n",
    "def normalize_tech_doc_id(doc_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes technical document IDs by removing spaces and converting to lowercase.\n",
    "    For example: 'MCE 0107 B' -> 'mce0107b'\n",
    "    \"\"\"\n",
    "    return ''.join(doc_id.split()).lower()\n",
    "\n",
    "def validate_tech_doc_recognition(nlp, text_input, manual_counts: Dict[str, int]) -> Tuple[PrettyTable, float]:\n",
    "    \"\"\"\n",
    "    Validates the NER model's performance on technical document recognition,\n",
    "    treating different format variations of the same document ID as equivalent.\n",
    "    \n",
    "    Args:\n",
    "        nlp: spaCy NLP model\n",
    "        text_input: Either a string or list of strings (chunks)\n",
    "        manual_counts: Dictionary of manual counts for each technical document\n",
    "    \n",
    "    Returns:\n",
    "        PrettyTable showing comparison results\n",
    "        Overall accuracy percentage\n",
    "    \"\"\"\n",
    "    # Handle different input types\n",
    "    if isinstance(text_input, list):\n",
    "        # Process each chunk individually and combine results\n",
    "        all_entities = []\n",
    "        \n",
    "        for chunk in text_input:\n",
    "            chunk_doc = nlp(chunk)\n",
    "            for ent in chunk_doc.ents:\n",
    "                if ent.label_ == \"TECH_DOC\":\n",
    "                    all_entities.append(ent)\n",
    "    else:\n",
    "        # Process as a single string\n",
    "        doc = nlp(text_input)\n",
    "        all_entities = [ent for ent in doc.ents if ent.label_ == \"TECH_DOC\"]\n",
    "    \n",
    "    # Process manual counts & normalize\n",
    "    normalized_manual_counts = {}\n",
    "    variations_map = {}\n",
    "    for original_id, count in manual_counts.items():\n",
    "        normalized_id = normalize_tech_doc_id(original_id)\n",
    "        if normalized_id not in normalized_manual_counts:\n",
    "            normalized_manual_counts[normalized_id] = 0\n",
    "            variations_map[normalized_id] = set()\n",
    "        normalized_manual_counts[normalized_id] += count\n",
    "        variations_map[normalized_id].add(original_id)\n",
    "    \n",
    "    # Count model predictions\n",
    "    predicted_counts = {}\n",
    "    for ent in all_entities:\n",
    "        normalized_ent = normalize_tech_doc_id(ent.text)\n",
    "        if normalized_ent not in predicted_counts:\n",
    "            predicted_counts[normalized_ent] = 0\n",
    "        predicted_counts[normalized_ent] += 1\n",
    "        if normalized_ent in variations_map:\n",
    "            variations_map[normalized_ent].add(ent.text)\n",
    "    \n",
    "    # Create comparison table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\n",
    "        \"Technical Document\",\n",
    "        \"Variations Found\",\n",
    "        \"Manual Count\",\n",
    "        \"Model Count\",\n",
    "        \"Difference\",\n",
    "        \"Accuracy %\"\n",
    "    ]\n",
    "    table.align = \"l\"\n",
    "    \n",
    "    # Track totals\n",
    "    total_manual = 0\n",
    "    total_predicted = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # Process all manual counts\n",
    "    processed_ids = set()\n",
    "    for normalized_id in normalized_manual_counts.keys():\n",
    "        if normalized_id in processed_ids:\n",
    "            continue\n",
    "            \n",
    "        processed_ids.add(normalized_id)\n",
    "        \n",
    "        manual_count = normalized_manual_counts[normalized_id]\n",
    "        predicted_count = predicted_counts.get(normalized_id, 0)\n",
    "        \n",
    "        # Get all variations found\n",
    "        variations = sorted(variations_map[normalized_id])\n",
    "        variations_str = \", \".join(variations)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = min(predicted_count, manual_count) / manual_count * 100 if manual_count > 0 else 0\n",
    "        \n",
    "        # Update totals\n",
    "        total_manual += manual_count\n",
    "        total_predicted += predicted_count\n",
    "        total_correct += min(predicted_count, manual_count)\n",
    "        \n",
    "        # Use the first variation as the primary ID for display\n",
    "        primary_id = sorted(variations_map[normalized_id])[0]\n",
    "        \n",
    "        table.add_row([\n",
    "            primary_id,\n",
    "            variations_str,\n",
    "            manual_count,\n",
    "            predicted_count,\n",
    "            predicted_count - manual_count,\n",
    "            f\"{accuracy:.1f}%\"\n",
    "        ])\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = (total_correct / total_manual * 100) if total_manual > 0 else 0\n",
    "    \n",
    "    # Add totals row\n",
    "    table.add_row([\n",
    "        \"TOTAL\",\n",
    "        \"\",\n",
    "        total_manual,\n",
    "        total_predicted,\n",
    "        total_predicted - total_manual,\n",
    "        f\"{overall_accuracy:.1f}%\"\n",
    "    ])\n",
    "    \n",
    "    return table, overall_accuracy\n",
    "# Run validation\n",
    "results_table, overall_accuracy = validate_tech_doc_recognition(nlp, big_chunk, manual_counts1)\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracies):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    pdfs = list(accuracies.keys())\n",
    "    acc_values = list(accuracies.values())\n",
    "    \n",
    "    bars = plt.bar(pdfs, acc_values, color='skyblue')\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Technical Document Recognition Accuracy Across PDFs', pad=20)\n",
    "    plt.xlabel('PDF Documents')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set y-axis range\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Add average line\n",
    "    avg_accuracy = np.mean(acc_values)\n",
    "    plt.axhline(y=avg_accuracy, color='r', linestyle='--', alpha=0.8)\n",
    "    plt.text(len(pdfs)-1, avg_accuracy, f'Average: {avg_accuracy:.1f}%', \n",
    "             va='bottom', ha='right', color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def main():\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nTechnical Document Recognition Validation\")\n",
    "    print(\"=\" * 80)\n",
    "    print(results_table)\n",
    "    print(f\"\\nOverall Model Accuracy: {overall_accuracy:.1f}%\")\n",
    "\n",
    "    # Get accuracies for all PDFs\n",
    "    accuracies = get_accuracies_for_all_pdfs(nlp)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_accuracies(accuracies)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_text_token(extracted_text: str) -> List[str]:\n",
    "    chunks = token_chunks_cl100k(extracted_text)\n",
    "    \n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "    \n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_text_recursive_char(extracted_text: str) -> List[str]:\n",
    "    chunks = recursive_character_chunks(extracted_text)\n",
    "    \n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "    \n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_text_recursive_token(extracted_text: str) -> List[str]:\n",
    "    chunks = recursive_token_chunks(extracted_text)\n",
    "    \n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_text_line(extracted_text: str) -> List[str]:\n",
    "    chunks = get_text_chunks2(extracted_text)\n",
    "    \n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "    \n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_text_semantic(extracted_text: str) -> List[str]:\n",
    "    chunks = semantic_cluster_chunks(extracted_text)\n",
    "    \n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "    \n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_text_structure(extracted_text: str) -> List[str]:\n",
    "    chunks = structure_aware_chunks(extracted_text)\n",
    "    \n",
    "    cleaned_chunks = []\n",
    "    for chunk in chunks:\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "    \n",
    "    return cleaned_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_man = [\n",
    "        (file1, manual_counts1, \"MCE0110B\"),\n",
    "        (file2, manual_counts2, \"MCE0107B\"),\n",
    "        (file3, manual_counts3, \"MCE1157E\"),\n",
    "        (file4, manual_counts4, \"MCG1093J\"),\n",
    "        (file5, manual_counts5, \"MCG1107B\"),\n",
    "        (file6, manual_counts6, \"MCG1202A\"),\n",
    "        (file7, manual_counts7, \"MCH1744H\"),\n",
    "        (file8, manual_counts8, \"MCH1759F\"),\n",
    "        (file9, manual_counts9, \"MCH1798H\"),\n",
    "        (file10, manual_counts10, \"MCH2629A\")\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "def compare_chunking_techniques(nlp, files_man):\n",
    "    \"\"\"\n",
    "    Compares the accuracy of different chunking techniques across multiple PDF files.\n",
    "    \n",
    "    Args:\n",
    "        nlp: The NLP pipeline with entity recognition\n",
    "        files_man: List of tuples with (file_path, manual_counts, pdf_name)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping chunking technique names to their average accuracies\n",
    "    \"\"\"\n",
    "    # Define all chunking techniques to compare\n",
    "    chunking_techniques = {\n",
    "        \"Standard Character\": process_pdf_text,\n",
    "        \"Line-by-Line\": process_pdf_text_line,\n",
    "        \"Token-based\": process_pdf_text_token,\n",
    "        \"Recursive Character\": process_pdf_text_recursive_char,\n",
    "        \"Structure-aware\": process_pdf_text_structure,\n",
    "        \"Semantic Clustering\": process_pdf_text_semantic\n",
    "    }\n",
    "    \n",
    "    # Store results for each technique\n",
    "    technique_results = {name: [] for name in chunking_techniques.keys()}\n",
    "    \n",
    "    # Process each PDF with each chunking technique\n",
    "    for file_path, manual_counts, pdf_name in files_man:\n",
    "        print(f\"\\nProcessing {pdf_name}...\")\n",
    "        \n",
    "        # Read PDF content (only once per file)\n",
    "        pdf_df = spark.read.format(\"binaryFile\").load(file_path).cache()\n",
    "        binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"]\n",
    "        extracted_text = extract_text(binary_pdf)\n",
    "        \n",
    "        # Test each chunking technique\n",
    "        for technique_name, chunking_function in chunking_techniques.items():\n",
    "            try:\n",
    "                print(f\"  Applying {technique_name} chunking...\")\n",
    "                # Ensure extracted_text is a string before passing to chunking function\n",
    "                if isinstance(extracted_text, list):\n",
    "                    text_to_process = \"\\n\".join(extracted_text)\n",
    "                else:\n",
    "                    text_to_process = extracted_text\n",
    "                chunks = chunking_function(extracted_text)\n",
    "                \n",
    "                # Validate recognition accuracy\n",
    "                _, accuracy = validate_tech_doc_recognition(nlp, chunks, manual_counts)\n",
    "                technique_results[technique_name].append((pdf_name, accuracy))\n",
    "            except Exception as e:\n",
    "                print(f\"    Error with {technique_name}: {str(e)}\")\n",
    "                # Record 0 accuracy on error to avoid skipping\n",
    "                technique_results[technique_name].append((pdf_name, 0.0))\n",
    "        \n",
    "        # Clear cache\n",
    "        pdf_df.unpersist()\n",
    "    \n",
    "    # Calculate average accuracies\n",
    "    average_accuracies = {}\n",
    "    for technique, results in technique_results.items():\n",
    "        if results:\n",
    "            accuracies = [acc for _, acc in results]\n",
    "            average_accuracies[technique] = np.mean(accuracies)\n",
    "        else:\n",
    "            average_accuracies[technique] = 0.0\n",
    "    \n",
    "    return technique_results, average_accuracies\n",
    "\n",
    "def plot_average_accuracies(average_accuracies):\n",
    "    \"\"\"\n",
    "    Creates a bar chart of average accuracies for different chunking techniques.\n",
    "    \n",
    "    Args:\n",
    "        average_accuracies: Dictionary mapping technique names to average accuracies\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Sort techniques by average accuracy\n",
    "    sorted_techniques = sorted(average_accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "    techniques = [t for t, _ in sorted_techniques]\n",
    "    accuracies = [a for _, a in sorted_techniques]\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = plt.bar(techniques, accuracies, color='skyblue')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Customize chart\n",
    "    plt.title('Average Accuracy by Chunking Technique', fontsize=15)\n",
    "    plt.xlabel('Chunking Technique', fontsize=12)\n",
    "    plt.ylabel('Average Accuracy (%)', fontsize=12)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def plot_technique_comparison_by_file(technique_results):\n",
    "    \"\"\"\n",
    "    Creates a grouped bar chart showing the accuracy of each technique for each PDF file.\n",
    "    \n",
    "    Args:\n",
    "        technique_results: Dictionary mapping technique names to lists of (pdf_name, accuracy) tuples\n",
    "    \"\"\"\n",
    "    # Get unique PDF names\n",
    "    pdf_names = sorted(set(pdf_name for technique in technique_results.values() \n",
    "                         for pdf_name, _ in technique))\n",
    "    \n",
    "    # Organize data by PDF\n",
    "    data_by_pdf = {pdf: {} for pdf in pdf_names}\n",
    "    for technique, results in technique_results.items():\n",
    "        for pdf_name, accuracy in results:\n",
    "            data_by_pdf[pdf_name][technique] = accuracy\n",
    "    \n",
    "    # Set up plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get technique names and set bar width\n",
    "    techniques = list(technique_results.keys())\n",
    "    num_techniques = len(techniques)\n",
    "    width = 0.8 / num_techniques\n",
    "    \n",
    "    # Create position indices for each PDF\n",
    "    indices = np.arange(len(pdf_names))\n",
    "    \n",
    "    # Plot grouped bars\n",
    "    for i, technique in enumerate(techniques):\n",
    "        # Get accuracies for this technique across all PDFs\n",
    "        accuracies = [data_by_pdf[pdf].get(technique, 0) for pdf in pdf_names]\n",
    "        \n",
    "        # Calculate bar positions\n",
    "        positions = indices + (i - num_techniques/2 + 0.5) * width\n",
    "        \n",
    "        # Plot bars for this technique\n",
    "        plt.bar(positions, accuracies, width, label=technique)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('PDF Document', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('Chunking Technique Comparison by PDF Document', fontsize=15)\n",
    "    plt.xticks(indices, pdf_names, rotation=45, ha='right')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend(title='Chunking Technique')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def create_heatmap(technique_results):\n",
    "    \"\"\"\n",
    "    Creates a heatmap showing the accuracy of each technique for each PDF file.\n",
    "    \n",
    "    Args:\n",
    "        technique_results: Dictionary mapping technique names to lists of (pdf_name, accuracy) tuples\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Get unique PDF names and technique names\n",
    "    pdf_names = sorted(set(pdf_name for technique in technique_results.values() \n",
    "                         for pdf_name, _ in technique))\n",
    "    techniques = list(technique_results.keys())\n",
    "    \n",
    "    # Create data matrix\n",
    "    data = np.zeros((len(pdf_names), len(techniques)))\n",
    "    for i, pdf in enumerate(pdf_names):\n",
    "        for j, technique in enumerate(techniques):\n",
    "            # Find accuracy for this PDF and technique\n",
    "            for pdf_name, accuracy in technique_results[technique]:\n",
    "                if pdf_name == pdf:\n",
    "                    data[i, j] = accuracy\n",
    "                    break\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(data, annot=True, fmt=\".1f\", \n",
    "                xticklabels=techniques, \n",
    "                yticklabels=pdf_names,\n",
    "                cmap=\"YlGnBu\", vmin=0, vmax=100)\n",
    "    \n",
    "    plt.title('Accuracy Heatmap: Chunking Techniques vs. PDF Documents', fontsize=15)\n",
    "    plt.xlabel('Chunking Technique', fontsize=12)\n",
    "    plt.ylabel('PDF Document', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Main execution function\n",
    "def evaluate_chunking_techniques(nlp, files_man):\n",
    "    \"\"\"\n",
    "    Main function to evaluate chunking techniques and generate visualizations.\n",
    "    \n",
    "    Args:\n",
    "        nlp: The NLP pipeline with entity recognition\n",
    "        files_man: List of tuples with (file_path, manual_counts, pdf_name)\n",
    "    \"\"\"\n",
    "    # Compare all techniques\n",
    "    technique_results, average_accuracies = compare_chunking_techniques(nlp, files_man)\n",
    "    \n",
    "    # Generate plots\n",
    "    avg_plot = plot_average_accuracies(average_accuracies)\n",
    "    comparison_plot = plot_technique_comparison_by_file(technique_results)\n",
    "    #heatmap_plot = create_heatmap(technique_results)\n",
    "    \n",
    "    # Display plots\n",
    "    avg_plot.show()\n",
    "    comparison_plot.show()\n",
    "    #heatmap_plot.show()\n",
    "    \n",
    "    return technique_results, average_accuracies\n",
    "\n",
    "# Example usage:\n",
    "evaluate_chunking_techniques(nlp, files_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "def compare_chunking_techniques_with_runtime(nlp, files_man):\n",
    "    \"\"\"\n",
    "    Compares the accuracy and execution time of different chunking techniques across multiple PDF files.\n",
    "    \n",
    "    Args:\n",
    "        nlp: The NLP pipeline with entity recognition\n",
    "        files_man: List of tuples with (file_path, manual_counts, pdf_name)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping chunking technique names to their average accuracies and runtimes\n",
    "    \"\"\"\n",
    "    # Define all chunking techniques to compare\n",
    "    chunking_techniques = {\n",
    "        \"Standard Character\": process_pdf_text,\n",
    "        \"Line-by-Line\": process_pdf_text_line,\n",
    "        \"Token-based\": process_pdf_text_token,\n",
    "        \"Recursive Character\": process_pdf_text_recursive_char,\n",
    "        \"Structure-aware\": process_pdf_text_structure,\n",
    "        \"Semantic Clustering\": process_pdf_text_semantic\n",
    "    }\n",
    "    \n",
    "    # Store results for each technique\n",
    "    technique_results = {name: [] for name in chunking_techniques.keys()}\n",
    "    technique_runtimes = {name: [] for name in chunking_techniques.keys()}\n",
    "    \n",
    "    # Process each PDF with each chunking technique\n",
    "    for file_path, manual_counts, pdf_name in files_man:\n",
    "        print(f\"\\nProcessing {pdf_name}...\")\n",
    "        \n",
    "        # Read PDF content (only once per file)\n",
    "        pdf_df = spark.read.format(\"binaryFile\").load(file_path).cache()\n",
    "        binary_pdf = pdf_df.select(\"content\").collect()[0][\"content\"]\n",
    "        extracted_text = extract_text(binary_pdf)\n",
    "        \n",
    "        # Test each chunking technique\n",
    "        for technique_name, chunking_function in chunking_techniques.items():\n",
    "            try:\n",
    "                print(f\"  Applying {technique_name} chunking...\")\n",
    "                \n",
    "                # Ensure text is in the right format\n",
    "                if isinstance(extracted_text, list):\n",
    "                    text_to_process = \"\\n\".join(extracted_text)\n",
    "                else:\n",
    "                    text_to_process = extracted_text\n",
    "                \n",
    "                # Measure execution time\n",
    "                start_time = time.time()\n",
    "                chunks = chunking_function(text_to_process)\n",
    "                chunking_time = time.time() - start_time\n",
    "                \n",
    "                # Measure NER execution time\n",
    "                start_time = time.time()\n",
    "                _, accuracy = validate_tech_doc_recognition(nlp, chunks, manual_counts)\n",
    "                ner_time = time.time() - start_time\n",
    "                \n",
    "                # Total processing time\n",
    "                total_time = chunking_time + ner_time\n",
    "                \n",
    "                # Store results\n",
    "                technique_results[technique_name].append((pdf_name, accuracy))\n",
    "                technique_runtimes[technique_name].append((pdf_name, total_time, chunking_time, ner_time))\n",
    "                \n",
    "                print(f\"    Accuracy: {accuracy:.1f}%\")\n",
    "                print(f\"    Runtime: {total_time:.2f}s (Chunking: {chunking_time:.2f}s, NER: {ner_time:.2f}s)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error with {technique_name}: {str(e)}\")\n",
    "                # Record 0 accuracy and -1 runtime on error to indicate failure\n",
    "                technique_results[technique_name].append((pdf_name, 0.0))\n",
    "                technique_runtimes[technique_name].append((pdf_name, -1, -1, -1))\n",
    "        \n",
    "        # Clear cache\n",
    "        pdf_df.unpersist()\n",
    "    \n",
    "    # Calculate average accuracies and runtimes\n",
    "    average_accuracies = {}\n",
    "    average_runtimes = {}\n",
    "    \n",
    "    for technique, results in technique_results.items():\n",
    "        if results:\n",
    "            accuracies = [acc for _, acc in results]\n",
    "            average_accuracies[technique] = np.mean(accuracies)\n",
    "        else:\n",
    "            average_accuracies[technique] = 0.0\n",
    "    \n",
    "    for technique, times in technique_runtimes.items():\n",
    "        if times:\n",
    "            # Filter out error cases (-1)\n",
    "            valid_times = [(pdf, t, c, n) for pdf, t, c, n in times if t >= 0]\n",
    "            if valid_times:\n",
    "                total_times = [t for _, t, _, _ in valid_times]\n",
    "                chunking_times = [c for _, _, c, _ in valid_times]\n",
    "                ner_times = [n for _, _, _, n in valid_times]\n",
    "                \n",
    "                average_runtimes[technique] = {\n",
    "                    'total': np.mean(total_times),\n",
    "                    'chunking': np.mean(chunking_times),\n",
    "                    'ner': np.mean(ner_times)\n",
    "                }\n",
    "            else:\n",
    "                average_runtimes[technique] = {'total': 0, 'chunking': 0, 'ner': 0}\n",
    "        else:\n",
    "            average_runtimes[technique] = {'total': 0, 'chunking': 0, 'ner': 0}\n",
    "    \n",
    "    return technique_results, technique_runtimes, average_accuracies, average_runtimes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_runtime_comparison_log_scale(average_runtimes):\n",
    "    \"\"\"\n",
    "    Creates a bar chart comparing the runtime of different chunking techniques\n",
    "    using a logarithmic scale to better show small values.\n",
    "    \n",
    "    Args:\n",
    "        average_runtimes: Dictionary mapping technique names to average runtime dictionaries\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Sort techniques by total runtime\n",
    "    sorted_techniques = sorted(average_runtimes.items(), key=lambda x: x[1]['total'])\n",
    "    techniques = [t for t, _ in sorted_techniques]\n",
    "    \n",
    "    # Extract the different time components\n",
    "    chunking_times = [r['chunking'] for _, r in sorted_techniques]\n",
    "    ner_times = [r['ner'] for _, r in sorted_techniques]\n",
    "    \n",
    "    # Set up the bar chart\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(techniques))\n",
    "    \n",
    "    # Create stacked bars\n",
    "    bars1 = plt.bar(index, chunking_times, bar_width, label='Chunking Time', color='skyblue')\n",
    "    bars2 = plt.bar(index, ner_times, bar_width, bottom=chunking_times, label='NER Time', color='lightcoral')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (chunking, ner) in enumerate(zip(chunking_times, ner_times)):\n",
    "        total = chunking + ner\n",
    "        plt.text(i, total * 1.05, f'{total:.2f}s', ha='center', va='bottom', fontsize=8)\n",
    "        # Add chunking time label inside or just above the chunking portion\n",
    "        plt.text(i, chunking * 0.5, f'{chunking:.3f}s', ha='center', va='center', \n",
    "                color='black', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # Customize chart\n",
    "    plt.title('Average Runtime by Chunking Technique (Log Scale)', fontsize=15)\n",
    "    plt.xlabel('Chunking Technique', fontsize=12)\n",
    "    plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "    plt.xticks(index, techniques, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set y-axis to logarithmic scale\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Add horizontal gridlines at specific values for better readability\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def plot_runtime_components_side_by_side(average_runtimes):\n",
    "    \"\"\"\n",
    "    Creates a side-by-side bar chart comparing chunking and NER times separately.\n",
    "    \n",
    "    Args:\n",
    "        average_runtimes: Dictionary mapping technique names to average runtime dictionaries\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Sort techniques by total runtime\n",
    "    sorted_techniques = sorted(average_runtimes.items(), key=lambda x: x[1]['total'])\n",
    "    techniques = [t for t, _ in sorted_techniques]\n",
    "    \n",
    "    # Extract the different time components\n",
    "    chunking_times = [r['chunking'] for _, r in sorted_techniques]\n",
    "    ner_times = [r['ner'] for _, r in sorted_techniques]\n",
    "    \n",
    "    # Set up the bar chart\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(techniques))\n",
    "    \n",
    "    # Create side-by-side bars\n",
    "    bar1 = plt.bar(index - bar_width/2, chunking_times, bar_width, label='Chunking Time', color='skyblue')\n",
    "    bar2 = plt.bar(index + bar_width/2, ner_times, bar_width, label='NER Time', color='lightcoral')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, time in enumerate(chunking_times):\n",
    "        plt.text(i - bar_width/2, time, f'{time:.3f}s', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "    for i, time in enumerate(ner_times):\n",
    "        plt.text(i + bar_width/2, time, f'{time:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Customize chart\n",
    "    plt.title('Runtime Components by Chunking Technique', fontsize=15)\n",
    "    plt.xlabel('Chunking Technique', fontsize=12)\n",
    "    plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "    plt.xticks(index, techniques, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technique_results, technique_runtimes, average_accuracies, average_runtimes = compare_chunking_techniques_with_runtime(nlp, files_man)\n",
    "\n",
    "log_plot = plot_runtime_comparison_log_scale(average_runtimes)\n",
    "side_plot = plot_runtime_components_side_by_side(average_runtimes)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
